{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/liuyu/anaconda3/lib/python3.7/site-packages/lightgbm/__init__.py:46: UserWarning: Starting from version 2.2.1, the library file in distribution wheels for macOS is built by the Apple Clang (Xcode_8.3.3) compiler.\n",
      "This means that in case of installing LightGBM from PyPI via the ``pip install lightgbm`` command, you don't need to install the gcc compiler anymore.\n",
      "Instead of that, you need to install the OpenMP library, which is required for running LightGBM on the system with the Apple Clang compiler.\n",
      "You can install the OpenMP library by the following command: ``brew install libomp``.\n",
      "  \"You can install the OpenMP library by the following command: ``brew install libomp``.\", UserWarning)\n"
     ]
    }
   ],
   "source": [
    "from datetime import date, timedelta\n",
    "import datetime\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm, tnrange\n",
    "\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import lightgbm as lgb\n",
    "\n",
    "import mlflow\n",
    "import mlflow.sklearn\n",
    "\n",
    "from config import (\n",
    "    RAW_DATA_DIR,\n",
    "    FEATURE_DIR,\n",
    "    LAG_DICT,\n",
    "    SLIDING_DICT\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# solve lightgbm error on MAC\n",
    "import os\n",
    "os.environ['KMP_DUPLICATE_LIB_OK']='True'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data\n",
    "df_train = pd.read_csv(\n",
    "    RAW_DATA_DIR+'train.csv', usecols=[1, 2, 3, 4, 5],\n",
    "    dtype={'onpromotion': bool},\n",
    "    converters={'unit_sales': lambda u: np.log1p(\n",
    "        float(u)) if float(u) > 0 else 0},\n",
    "    parse_dates=[\"date\"],\n",
    "    skiprows=range(1, 66458909)  # 2016-01-01\n",
    ")\n",
    "\n",
    "df_test = pd.read_csv(\n",
    "    RAW_DATA_DIR+'test.csv', usecols=[0, 1, 2, 3, 4],\n",
    "    dtype={'onpromotion': bool},\n",
    "    parse_dates=[\"date\"]  # , date_parser=parser\n",
    ").set_index(\n",
    "    ['store_nbr', 'item_nbr', 'date']\n",
    ")\n",
    "\n",
    "items = pd.read_csv(\n",
    "    RAW_DATA_DIR+'items.csv',\n",
    ").set_index(\"item_nbr\")\n",
    "\n",
    "stores = pd.read_csv(\n",
    "    RAW_DATA_DIR+'stores.csv',\n",
    ").set_index(\"store_nbr\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test Period\n",
    "\n",
    "2017-08-16 to 2017-08-31"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_start = date(2017, 8, 16)\n",
    "test_end = date(2017,8, 31)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "valid starts from 2017-07-26 to 2017-08-10\n"
     ]
    }
   ],
   "source": [
    "valid_start = test_start - timedelta(16)\n",
    "while(1):\n",
    "    if valid_start.weekday() == test_start.weekday():\n",
    "        break\n",
    "    valid_start = valid_start-timedelta(days=1)\n",
    "valid_end = valid_start + timedelta(15)\n",
    "print('valid starts from {} to {}'.format(valid_start, valid_end))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Valid Period\n",
    "\n",
    "Considering the more nearer peiods of sales data may have more in common, it would be better to find the nearest period as valid period.\n",
    "\n",
    "Based on the analysis before, we assume the sales data is periodically with the frequency of 7 days, so we want to keep that feature same\n",
    "in the train, valid and test period.\n",
    "\n",
    "So finally, we choose valid period:\n",
    "\n",
    "2017-07-26 to 2017-08-10\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_start = date(2017, 7, 26)\n",
    "valid_end = date(2017, 8, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filter Period\n",
    "\n",
    "#### Earthquake happended on April 16, 2016. It may affect for the next several weeks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train datasets starts from 2016-10-05\n"
     ]
    }
   ],
   "source": [
    "filter_date = date(2016,4,16) + timedelta(7*4)\n",
    "lag_max = 140\n",
    "train_start=  filter_date+timedelta(days=lag_max)\n",
    "\n",
    "while(1):\n",
    "    train_start = train_start + timedelta(1)\n",
    "    if train_start.weekday() == valid_start.weekday():\n",
    "        break\n",
    "print('train datasets starts from {}'.format(train_start))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Wages in the public sector are paid every two weeks on the 15 th and on the last day of the month. Supermarket sales could be affected by this.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/liuyu/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:1: FutureWarning: Comparing Series of datetimes with 'datetime.date'.  Currently, the\n",
      "'datetime.date' is coerced to a datetime. In the future pandas will\n",
      "not coerce, and a TypeError will be raised. To retain the current\n",
      "behavior, convert the 'datetime.date' to a datetime with\n",
      "'pd.Timestamp'.\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    }
   ],
   "source": [
    "df_train = df_train[df_train['date']>=filter_date]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Promo feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "promo_train = df_train.set_index(\n",
    "    [\"store_nbr\", \"item_nbr\", \"date\"])[[\"onpromotion\"]]\n",
    "\n",
    "# missing onpromotions filling\n",
    "promo_train = promo_train.unstack(level=-1).fillna(False)\n",
    "promo_train.columns = promo_train.columns.get_level_values(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# missing test onpromotions filling\n",
    "promo_test = df_test[[\"onpromotion\"]].unstack(level=-1).fillna(False)\n",
    "promo_test.columns = promo_test.columns.get_level_values(1)\n",
    "# filter those items/stores in promo_test but not in promo_train\n",
    "promo_test = promo_test.reindex(promo_train.index).fillna(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "promo_features = pd.concat([promo_train, promo_test], axis=1)\n",
    "del promo_test, promo_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# label\n",
    "df_train = df_train.set_index([\"store_nbr\", \"item_nbr\", \"date\"])[[\"unit_sales\"]].unstack(level=-1).fillna(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Item"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "items = items.reindex(df_train.index.get_level_values(1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Item Family"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "items['family'] = items['family'].astype('category')\n",
    "item_family_features = items.family.cat.codes.values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Item's class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "items['class'] = items['class'].astype('category')\n",
    "item_class_features = items['class'].cat.codes.values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "stores = stores.reindex(df_train.index.get_level_values(0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Store's city"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "stores['city'] = stores['city'].astype('category')\n",
    "store_city_features = stores['city'].cat.codes.values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Store's state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "stores['state'] = stores['state'].astype('category')\n",
    "store_state_features = stores['state'].cat.codes.values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Store's type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "stores['type'] = stores['type'].astype('category')\n",
    "store_type_features = stores['type'].cat.codes.values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Store's cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "stores['cluster'] = stores['cluster'].astype('category')\n",
    "store_cluster_features = stores['cluster'].cat.codes.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.columns = df_train.columns.get_level_values(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Filling missing date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2016-12-25 00:00:00\n"
     ]
    }
   ],
   "source": [
    "date_list = df_train.columns\n",
    "obj_list = pd.date_range(filter_date, test_start-timedelta(1))\n",
    "diff_list = list(set(obj_list) - set(date_list)) \n",
    "for i in diff_list:\n",
    "    print(i)\n",
    "    df_train[i] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2016-12-25 00:00:00\n"
     ]
    }
   ],
   "source": [
    "date_list = promo_features.columns\n",
    "obj_list = pd.date_range(filter_date, test_end)\n",
    "diff_list = list(set(obj_list) - set(date_list)) \n",
    "for i in diff_list:\n",
    "    print(i)\n",
    "    promo_features[i] = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Lagging and sliding windows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "LAG_DICT = {'unit_sales': [1,2,3,4,5,6,7,14,21,28,35,42,49,56,63],\n",
    "            'onpromotion': [2, 3,4,5,6, 7, 14, 21]}\n",
    "\n",
    "SLIDING_DICT = {'unit_sales': [3, 4, 5, 6, 7, 14, 21, 30, 60, 63]}\n",
    "\n",
    "# initialise dirs\n",
    "RAW_DATA_DIR = 'datasets/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_timespan(df, \n",
    "                 start_time,\n",
    "                 minus,\n",
    "                 periods,\n",
    "                 freq='D'):\n",
    "    return df[pd.date_range(start_time - timedelta(days=minus), periods=periods, freq=freq)]\n",
    "\n",
    "def gen_dataset(df, \n",
    "                promo_features,\n",
    "                item_family_features,\n",
    "                item_class_features,\n",
    "                store_city_features,\n",
    "                store_state_features,\n",
    "                store_type_features,\n",
    "                store_cluster_features,\n",
    "                start_time,\n",
    "                is_train=True):\n",
    "    # init\n",
    "    X = pd.DataFrame()\n",
    "    \n",
    "    for i in LAG_DICT['unit_sales']:\n",
    "        X['lag_{}_sales'.format(i)] = get_timespan(df, start_time, i, 1).values.ravel()\n",
    "    \n",
    "    for i in LAG_DICT['onpromotion']:\n",
    "        X['sum_{}_promo'.format(i)] = get_timespan(promo_features, start_time, i, 1).sum(axis=1).ravel()\n",
    "\n",
    "    for i in SLIDING_DICT['unit_sales']:\n",
    "        X[\"mean_{}_sales\".format(i)] = get_timespan(df, start_time, i, i).mean(axis=1).values\n",
    "        X[\"std_{}_sales\".format(i)] = get_timespan(df, start_time, i, i).std(axis=1).values\n",
    "\n",
    "    for i in range(7):\n",
    "        X['mean_4_dow{}_2017'.format(i)] = get_timespan(df, start_time, 28-i, 4, freq='7D').mean(axis=1).values\n",
    "        X['mean_20_dow{}_2017'.format(i)] = get_timespan(df, start_time, 140-i, 20, freq='7D').mean(axis=1).values\n",
    "        \n",
    "    # for the next to-predict 16 days \n",
    "    for i in range(16):\n",
    "        X[\"promo_{}\".format(i)] = promo_features[start_time + timedelta(days=i)].values.astype(np.uint8)\n",
    "\n",
    "    X['item_family_features'] = item_family_features\n",
    "\n",
    "    X['item_class_features'] = item_class_features\n",
    "\n",
    "    X['store_city_features'] = store_city_features\n",
    "\n",
    "    X['store_state_features'] = store_state_features\n",
    "\n",
    "    X['store_type_features'] = store_type_features\n",
    "\n",
    "    X['store_cluster_features'] = store_cluster_features\n",
    "        \n",
    "    if is_train:\n",
    "        y = df[pd.date_range(start_time, periods=16)].values\n",
    "        return X, y\n",
    "    return X\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generate train, valid and test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "No. of week:   0%|          | 0/42 [00:00<?, ?it/s]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing dataset...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "No. of week:   2%|▏         | 1/42 [00:01<00:48,  1.19s/it]\u001b[A\n",
      "No. of week:   5%|▍         | 2/42 [00:01<00:42,  1.07s/it]\u001b[A\n",
      "No. of week:   7%|▋         | 3/42 [00:02<00:37,  1.03it/s]\u001b[A\n",
      "No. of week:  10%|▉         | 4/42 [00:03<00:34,  1.12it/s]\u001b[A\n",
      "No. of week:  12%|█▏        | 5/42 [00:04<00:31,  1.18it/s]\u001b[A\n",
      "No. of week:  14%|█▍        | 6/42 [00:04<00:29,  1.24it/s]\u001b[A\n",
      "No. of week:  17%|█▋        | 7/42 [00:05<00:27,  1.27it/s]\u001b[A\n",
      "No. of week:  19%|█▉        | 8/42 [00:06<00:26,  1.29it/s]\u001b[A\n",
      "No. of week:  21%|██▏       | 9/42 [00:07<00:25,  1.30it/s]\u001b[A\n",
      "No. of week:  24%|██▍       | 10/42 [00:07<00:24,  1.31it/s]\u001b[A\n",
      "No. of week:  26%|██▌       | 11/42 [00:08<00:23,  1.31it/s]\u001b[A\n",
      "No. of week:  29%|██▊       | 12/42 [00:09<00:22,  1.31it/s]\u001b[A\n",
      "No. of week:  31%|███       | 13/42 [00:10<00:24,  1.21it/s]\u001b[A\n",
      "No. of week:  33%|███▎      | 14/42 [00:11<00:24,  1.13it/s]\u001b[A\n",
      "No. of week:  36%|███▌      | 15/42 [00:12<00:24,  1.09it/s]\u001b[A\n",
      "No. of week:  38%|███▊      | 16/42 [00:13<00:24,  1.07it/s]\u001b[A\n",
      "No. of week:  40%|████      | 17/42 [00:14<00:23,  1.07it/s]\u001b[A\n",
      "No. of week:  43%|████▎     | 18/42 [00:15<00:22,  1.07it/s]\u001b[A\n",
      "No. of week:  45%|████▌     | 19/42 [00:16<00:21,  1.07it/s]\u001b[A\n",
      "No. of week:  48%|████▊     | 20/42 [00:17<00:20,  1.06it/s]\u001b[A\n",
      "No. of week:  50%|█████     | 21/42 [00:18<00:19,  1.05it/s]\u001b[A\n",
      "No. of week:  52%|█████▏    | 22/42 [00:18<00:17,  1.11it/s]\u001b[A\n",
      "No. of week:  55%|█████▍    | 23/42 [00:19<00:16,  1.17it/s]\u001b[A\n",
      "No. of week:  57%|█████▋    | 24/42 [00:20<00:14,  1.20it/s]\u001b[A\n",
      "No. of week:  60%|█████▉    | 25/42 [00:21<00:13,  1.23it/s]\u001b[A\n",
      "No. of week:  62%|██████▏   | 26/42 [00:21<00:12,  1.24it/s]\u001b[A\n",
      "No. of week:  64%|██████▍   | 27/42 [00:22<00:11,  1.26it/s]\u001b[A\n",
      "No. of week:  67%|██████▋   | 28/42 [00:23<00:11,  1.25it/s]\u001b[A\n",
      "No. of week:  69%|██████▉   | 29/42 [00:24<00:10,  1.25it/s]\u001b[A\n",
      "No. of week:  71%|███████▏  | 30/42 [00:25<00:09,  1.26it/s]\u001b[A\n",
      "No. of week:  74%|███████▍  | 31/42 [00:25<00:08,  1.27it/s]\u001b[A\n",
      "No. of week:  76%|███████▌  | 32/42 [00:26<00:07,  1.28it/s]\u001b[A\n",
      "No. of week:  79%|███████▊  | 33/42 [00:27<00:06,  1.29it/s]\u001b[A\n",
      "No. of week:  81%|████████  | 34/42 [00:28<00:06,  1.27it/s]\u001b[A\n",
      "No. of week:  83%|████████▎ | 35/42 [00:29<00:05,  1.28it/s]\u001b[A\n",
      "No. of week:  86%|████████▌ | 36/42 [00:29<00:04,  1.28it/s]\u001b[A\n",
      "No. of week:  88%|████████▊ | 37/42 [00:30<00:03,  1.29it/s]\u001b[A\n",
      "No. of week:  90%|█████████ | 38/42 [00:31<00:03,  1.29it/s]\u001b[A\n",
      "No. of week:  93%|█████████▎| 39/42 [00:32<00:02,  1.30it/s]\u001b[A\n",
      "No. of week:  95%|█████████▌| 40/42 [00:32<00:01,  1.30it/s]\u001b[A\n",
      "No. of week:  98%|█████████▊| 41/42 [00:33<00:00,  1.29it/s]\u001b[A\n",
      "No. of week: 100%|██████████| 42/42 [00:34<00:00,  1.30it/s]\u001b[A"
     ]
    }
   ],
   "source": [
    "print(\"Preparing dataset...\")\n",
    "\n",
    "nbr_weeks = int((valid_start - train_start).days/7)\n",
    "\n",
    "X_l, y_l = [], []\n",
    "\n",
    "for i in tqdm(range(nbr_weeks), desc = 'No. of week'):\n",
    "    delta = timedelta(days=7 * i)\n",
    "    X_tmp, y_tmp = gen_dataset(\n",
    "        df_train,\n",
    "        promo_features,\n",
    "        item_family_features,\n",
    "        item_class_features,\n",
    "        store_city_features,\n",
    "        store_state_features,\n",
    "        store_type_features,\n",
    "        store_cluster_features,\n",
    "        train_start + delta\n",
    "    )\n",
    "    X_l.append(X_tmp)\n",
    "    y_l.append(y_tmp)\n",
    "#     break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = pd.concat(X_l, axis=0)\n",
    "y_train = np.concatenate(y_l, axis=0)\n",
    "del X_l, y_l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "X_val, y_val = gen_dataset(df_train,\n",
    "                           promo_features,\n",
    "                           item_family_features,\n",
    "                           item_class_features,\n",
    "                           store_city_features,\n",
    "                           store_state_features,\n",
    "                           store_type_features,\n",
    "                           store_cluster_features,\n",
    "                           valid_start)\n",
    "X_test = gen_dataset(df_train, \n",
    "                    promo_features,\n",
    "                    item_family_features,\n",
    "                    item_class_features,\n",
    "                    store_city_features,\n",
    "                    store_state_features,\n",
    "                    store_type_features,\n",
    "                    store_cluster_features,\n",
    "                    test_start, is_train=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training and predicting models...\n"
     ]
    }
   ],
   "source": [
    "print(\"Training and predicting models...\")\n",
    "params = {\n",
    "    'num_leaves': 2**5 - 1,\n",
    "    'objective': 'regression_l2',\n",
    "    'max_depth': 8,\n",
    "    'min_data_in_leaf': 50,\n",
    "    'learning_rate': 0.05,\n",
    "    'feature_fraction': 0.75,\n",
    "    'bagging_fraction': 0.75,\n",
    "    'bagging_freq': 1,\n",
    "    'metric': 'l2',\n",
    "    'num_threads': 4\n",
    "}\n",
    "\n",
    "MAX_ROUNDS = 200\n",
    "val_pred = []\n",
    "test_pred = []\n",
    "cate_vars = ['item_family_features',\n",
    "            'item_class_features',\n",
    "            'store_city_features',\n",
    "            'store_state_features',\n",
    "            'store_type_features',\n",
    "            'store_cluster_features']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|          | 0/16 [00:00<?, ?it/s]\u001b[A/Users/liuyu/anaconda3/lib/python3.7/site-packages/lightgbm/basic.py:762: UserWarning: categorical_feature in param dict is overridden.\n",
      "  warnings.warn('categorical_feature in param dict is overridden.')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1]\ttraining's l2: 1.0669\tvalid_1's l2: 1.00319\n",
      "Training until validation scores don't improve for 50 rounds.\n",
      "[2]\ttraining's l2: 0.997115\tvalid_1's l2: 0.936875\n",
      "[3]\ttraining's l2: 0.93291\tvalid_1's l2: 0.8756\n",
      "[4]\ttraining's l2: 0.875641\tvalid_1's l2: 0.821276\n",
      "[5]\ttraining's l2: 0.82264\tvalid_1's l2: 0.77096\n",
      "[6]\ttraining's l2: 0.774825\tvalid_1's l2: 0.725402\n",
      "[7]\ttraining's l2: 0.731454\tvalid_1's l2: 0.684427\n",
      "[8]\ttraining's l2: 0.692291\tvalid_1's l2: 0.647398\n",
      "[9]\ttraining's l2: 0.656942\tvalid_1's l2: 0.614013\n",
      "[10]\ttraining's l2: 0.624789\tvalid_1's l2: 0.58376\n",
      "[11]\ttraining's l2: 0.595655\tvalid_1's l2: 0.55643\n",
      "[12]\ttraining's l2: 0.569987\tvalid_1's l2: 0.532496\n",
      "[13]\ttraining's l2: 0.546698\tvalid_1's l2: 0.510864\n",
      "[14]\ttraining's l2: 0.524965\tvalid_1's l2: 0.49061\n",
      "[15]\ttraining's l2: 0.505157\tvalid_1's l2: 0.472122\n",
      "[16]\ttraining's l2: 0.487249\tvalid_1's l2: 0.455509\n",
      "[17]\ttraining's l2: 0.471025\tvalid_1's l2: 0.440529\n",
      "[18]\ttraining's l2: 0.45627\tvalid_1's l2: 0.426768\n",
      "[19]\ttraining's l2: 0.44289\tvalid_1's l2: 0.414437\n",
      "[20]\ttraining's l2: 0.430698\tvalid_1's l2: 0.403226\n",
      "[21]\ttraining's l2: 0.419676\tvalid_1's l2: 0.393144\n",
      "[22]\ttraining's l2: 0.409646\tvalid_1's l2: 0.384057\n",
      "[23]\ttraining's l2: 0.400875\tvalid_1's l2: 0.3761\n",
      "[24]\ttraining's l2: 0.392462\tvalid_1's l2: 0.368477\n",
      "[25]\ttraining's l2: 0.384837\tvalid_1's l2: 0.36166\n",
      "[26]\ttraining's l2: 0.377978\tvalid_1's l2: 0.355429\n",
      "[27]\ttraining's l2: 0.371988\tvalid_1's l2: 0.350148\n",
      "[28]\ttraining's l2: 0.366536\tvalid_1's l2: 0.345378\n",
      "[29]\ttraining's l2: 0.361232\tvalid_1's l2: 0.34059\n",
      "[30]\ttraining's l2: 0.356219\tvalid_1's l2: 0.336185\n",
      "[31]\ttraining's l2: 0.351822\tvalid_1's l2: 0.332169\n",
      "[32]\ttraining's l2: 0.347627\tvalid_1's l2: 0.328525\n",
      "[33]\ttraining's l2: 0.344239\tvalid_1's l2: 0.325595\n",
      "[34]\ttraining's l2: 0.341011\tvalid_1's l2: 0.322924\n",
      "[35]\ttraining's l2: 0.337903\tvalid_1's l2: 0.320174\n",
      "[36]\ttraining's l2: 0.334926\tvalid_1's l2: 0.317564\n",
      "[37]\ttraining's l2: 0.332216\tvalid_1's l2: 0.31521\n",
      "[38]\ttraining's l2: 0.329848\tvalid_1's l2: 0.313128\n",
      "[39]\ttraining's l2: 0.327545\tvalid_1's l2: 0.311188\n",
      "[40]\ttraining's l2: 0.325451\tvalid_1's l2: 0.309403\n",
      "[41]\ttraining's l2: 0.323719\tvalid_1's l2: 0.308012\n",
      "[42]\ttraining's l2: 0.321949\tvalid_1's l2: 0.306492\n",
      "[43]\ttraining's l2: 0.320493\tvalid_1's l2: 0.305349\n",
      "[44]\ttraining's l2: 0.319182\tvalid_1's l2: 0.30432\n",
      "[45]\ttraining's l2: 0.317748\tvalid_1's l2: 0.303095\n",
      "[46]\ttraining's l2: 0.316449\tvalid_1's l2: 0.301938\n",
      "[47]\ttraining's l2: 0.315238\tvalid_1's l2: 0.300862\n",
      "[48]\ttraining's l2: 0.31415\tvalid_1's l2: 0.299955\n",
      "[49]\ttraining's l2: 0.313182\tvalid_1's l2: 0.2992\n",
      "[50]\ttraining's l2: 0.312405\tvalid_1's l2: 0.298611\n",
      "[51]\ttraining's l2: 0.311635\tvalid_1's l2: 0.298042\n",
      "[52]\ttraining's l2: 0.310779\tvalid_1's l2: 0.297349\n"
     ]
    }
   ],
   "source": [
    "for i in tqdm(range(16)):\n",
    "    dtrain = lgb.Dataset(\n",
    "        X_train, label=y_train[:, i],\n",
    "        categorical_feature=cate_vars,\n",
    "        weight=pd.concat([items[\"perishable\"]] * nbr_weeks) * 0.25 + 1\n",
    "    )\n",
    "    dval = lgb.Dataset(\n",
    "        X_val, label=y_val[:, i], reference=dtrain,\n",
    "        weight=items[\"perishable\"] * 0.25 + 1,\n",
    "        categorical_feature=cate_vars)\n",
    "\n",
    "    bst = lgb.train(\n",
    "        params,\n",
    "        dtrain,\n",
    "        num_boost_round=MAX_ROUNDS,\n",
    "#         verbose_eval = False,\n",
    "        valid_sets=[dtrain, dval], early_stopping_rounds=50)\n",
    "    print(\"\\n\".join((\"%s: %.2f\" % x) for x in sorted(\n",
    "        zip(X_train.columns, bst.feature_importance(\"gain\")),\n",
    "        key=lambda x: x[1], reverse=True\n",
    "    )))\n",
    "    val_pred.append(bst.predict(\n",
    "        X_val, num_iteration=bst.best_iteration or MAX_ROUNDS))\n",
    "    test_pred.append(bst.predict(\n",
    "        X_test, num_iteration=bst.best_iteration or MAX_ROUNDS))\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generate submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation mse: 0.3594203168120458\n",
      "Making submission...\n"
     ]
    }
   ],
   "source": [
    "mse = mean_squared_error(y_val, np.array(val_pred).transpose())\n",
    "\n",
    "mlflow.set_experiment('grocery forecasting')\n",
    "with mlflow.start_run(run_name='lgbm'):\n",
    "    mlflow.log_param('model', 'lgbm')\n",
    "    mlflow.log_param('train starts', train_start)\n",
    "    mlflow.log_params(params)\n",
    "    mlflow.log_params('lagging': LAG_DICT.values)\n",
    "    mlflow.log_params('slidingWindows': SLIDING_DICT.values)\n",
    "    mlflow.log_metric('mse', mse)\n",
    "    \n",
    "print(\"Validation mse:\", mse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Making submission...\")\n",
    "y_test = np.array(test_pred).transpose()\n",
    "df_preds = pd.DataFrame(\n",
    "    y_test, index=df_train.index,\n",
    "    columns=pd.date_range(\"2017-08-16\", periods=16)\n",
    ").stack().to_frame(\"unit_sales\")\n",
    "df_preds.index.set_names([\"store_nbr\", \"item_nbr\", \"date\"], inplace=True)\n",
    "\n",
    "submission = df_test[[\"id\"]].join(df_preds, how=\"left\").fillna(0)\n",
    "submission[\"unit_sales\"] = np.clip(np.expm1(submission[\"unit_sales\"]), 0, 1000)\n",
    "submission.to_csv('lgb.csv', float_format='%.4f', index=None)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
