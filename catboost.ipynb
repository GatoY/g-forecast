{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/liuyu/anaconda3/lib/python3.7/site-packages/lightgbm/__init__.py:46: UserWarning: Starting from version 2.2.1, the library file in distribution wheels for macOS is built by the Apple Clang (Xcode_8.3.3) compiler.\n",
      "This means that in case of installing LightGBM from PyPI via the ``pip install lightgbm`` command, you don't need to install the gcc compiler anymore.\n",
      "Instead of that, you need to install the OpenMP library, which is required for running LightGBM on the system with the Apple Clang compiler.\n",
      "You can install the OpenMP library by the following command: ``brew install libomp``.\n",
      "  \"You can install the OpenMP library by the following command: ``brew install libomp``.\", UserWarning)\n"
     ]
    }
   ],
   "source": [
    "from datetime import date, timedelta\n",
    "import datetime\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm, tnrange\n",
    "\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import lightgbm as lgb\n",
    "\n",
    "import mlflow\n",
    "import mlflow.sklearn\n",
    "\n",
    "from config import (\n",
    "    RAW_DATA_DIR,\n",
    "    FEATURE_DIR,\n",
    "    LAG_DICT,\n",
    "    SLIDING_DICT\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# solve lightgbm error on MAC\n",
    "import os\n",
    "os.environ['KMP_DUPLICATE_LIB_OK']='True'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data\n",
    "df_train = pd.read_csv(\n",
    "    RAW_DATA_DIR+'train.csv', usecols=[1, 2, 3, 4, 5],\n",
    "    dtype={'onpromotion': bool},\n",
    "    converters={'unit_sales': lambda u: np.log1p(\n",
    "        float(u)) if float(u) > 0 else 0},\n",
    "    parse_dates=[\"date\"],\n",
    "    skiprows=range(1, 66458909)  # 2016-01-01\n",
    ")\n",
    "\n",
    "df_test = pd.read_csv(\n",
    "    RAW_DATA_DIR+'test.csv', usecols=[0, 1, 2, 3, 4],\n",
    "    dtype={'onpromotion': bool},\n",
    "    parse_dates=[\"date\"]  # , date_parser=parser\n",
    ").set_index(\n",
    "    ['store_nbr', 'item_nbr', 'date']\n",
    ")\n",
    "\n",
    "items = pd.read_csv(\n",
    "    RAW_DATA_DIR+'items.csv',\n",
    ").set_index(\"item_nbr\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test Period\n",
    "\n",
    "2017-08-16 to 2017-08-31"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_start = date(2017, 8, 16)\n",
    "test_end = date(2017,8, 31)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "valid starts from 2017-07-26 to 2017-08-10\n"
     ]
    }
   ],
   "source": [
    "\n",
    "valid_start = test_start - timedelta(16)\n",
    "while(1):\n",
    "    if valid_start.weekday() == test_start.weekday():\n",
    "        break\n",
    "    valid_start = valid_start-timedelta(days=1)\n",
    "valid_end = valid_start + timedelta(15)\n",
    "print('valid starts from {} to {}'.format(valid_start, valid_end))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Valid Period\n",
    "\n",
    "Considering the more nearer peiods of sales data may have more in common, it would be better to find the nearest period as valid period.\n",
    "\n",
    "Based on the analysis before, we assume the sales data is periodically with the frequency of 7 days, so we want to keep that feature same\n",
    "in the train, valid and test period.\n",
    "\n",
    "So finally, we choose valid period:\n",
    "\n",
    "2017-07-26 to 2017-08-10\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_start = date(2017, 7, 26)\n",
    "valid_end = date(2017, 8, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filter Period\n",
    "\n",
    "#### Earthquake happended on April 16, 2016. It may affect for the next several weeks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train datasets starts from 2016-10-05\n"
     ]
    }
   ],
   "source": [
    "filter_date = date(2016,4,16) + timedelta(7*4)\n",
    "lag_max = 140\n",
    "train_start=  filter_date+timedelta(days=lag_max)\n",
    "\n",
    "while(1):\n",
    "    train_start = train_start + timedelta(1)\n",
    "    if train_start.weekday() == valid_start.weekday():\n",
    "        break\n",
    "print('train datasets starts from {}'.format(train_start))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Wages in the public sector are paid every two weeks on the 15 th and on the last day of the month. Supermarket sales could be affected by this.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/liuyu/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:1: FutureWarning: Comparing Series of datetimes with 'datetime.date'.  Currently, the\n",
      "'datetime.date' is coerced to a datetime. In the future pandas will\n",
      "not coerce, and a TypeError will be raised. To retain the current\n",
      "behavior, convert the 'datetime.date' to a datetime with\n",
      "'pd.Timestamp'.\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    }
   ],
   "source": [
    "df_train = df_train[df_train['date']>=filter_date]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Promo feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "promo_train = df_train.set_index(\n",
    "    [\"store_nbr\", \"item_nbr\", \"date\"])[[\"onpromotion\"]]\n",
    "\n",
    "# missing onpromotions filling\n",
    "promo_train = promo_train.unstack(level=-1).fillna(False)\n",
    "promo_train.columns = promo_train.columns.get_level_values(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# missing test onpromotions filling\n",
    "promo_test = df_test[[\"onpromotion\"]].unstack(level=-1).fillna(False)\n",
    "promo_test.columns = promo_test.columns.get_level_values(1)\n",
    "# filter those items/stores in promo_test but not in promo_train\n",
    "promo_test = promo_test.reindex(promo_train.index).fillna(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "promo_features = pd.concat([promo_train, promo_test], axis=1)\n",
    "del promo_test, promo_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# label\n",
    "df_train = df_train.set_index([\"store_nbr\", \"item_nbr\", \"date\"])[[\"unit_sales\"]].unstack(level=-1).fillna(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Perishable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "items = items.reindex(df_train.index.get_level_values(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.columns = df_train.columns.get_level_values(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Filling missing date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2016-12-25 00:00:00\n"
     ]
    }
   ],
   "source": [
    "date_list = df_train.columns\n",
    "obj_list = pd.date_range(filter_date, test_start-timedelta(1))\n",
    "diff_list = list(set(obj_list) - set(date_list)) \n",
    "for i in diff_list:\n",
    "    print(i)\n",
    "    df_train[i] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2016-12-25 00:00:00\n"
     ]
    }
   ],
   "source": [
    "date_list = promo_features.columns\n",
    "obj_list = pd.date_range(filter_date, test_end)\n",
    "diff_list = list(set(obj_list) - set(date_list)) \n",
    "for i in diff_list:\n",
    "    print(i)\n",
    "    promo_features[i] = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Lagging and sliding windows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "LAG_DICT = {'unit_sales': [1,2,5,7,14,21,28,35],\n",
    "            'onpromotion': [14, 60]}\n",
    "\n",
    "SLIDING_DICT = {'unit_sales': [3, 7, 14, 30, 60]}\n",
    "\n",
    "# initialise dirs\n",
    "RAW_DATA_DIR = 'datasets/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_timespan(df, \n",
    "                 start_time,\n",
    "                 minus,\n",
    "                 periods,\n",
    "                 freq='D'):\n",
    "    return df[pd.date_range(start_time - timedelta(days=minus), periods=periods, freq=freq)]\n",
    "\n",
    "def gen_dataset(df, \n",
    "                promo_features,\n",
    "                start_time,\n",
    "                is_train=True):\n",
    "    # init\n",
    "    X = pd.DataFrame()\n",
    "    \n",
    "    for i in LAG_DICT['unit_sales']:\n",
    "        X['lag_{}_sales'.format(i)] = get_timespan(df, start_time, i, 1).values.ravel()\n",
    "    \n",
    "    for i in LAG_DICT['onpromotion']:\n",
    "        X['sum_{}_promo'.format(i)] = get_timespan(promo_features, start_time, i, 1).sum(axis=1).ravel()\n",
    "\n",
    "    for i in SLIDING_DICT['unit_sales']:\n",
    "        X[\"mean_{}_sales\".format(i)] = get_timespan(df, start_time, i, i).mean(axis=1).values\n",
    "\n",
    "    # for the next to-predict 16 days \n",
    "    for i in range(16):\n",
    "        X[\"promo_{}\".format(i)] = promo_features[start_time + timedelta(days=i)].values.astype(np.uint8)\n",
    "\n",
    "    if is_train:\n",
    "        y = df[pd.date_range(start_time, periods=16)].values\n",
    "        return X, y\n",
    "    return X\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generate train, valid and test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No. of week:   0%|          | 0/42 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing dataset...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No. of week: 100%|██████████| 42/42 [00:08<00:00,  4.81it/s]\n"
     ]
    }
   ],
   "source": [
    "print(\"Preparing dataset...\")\n",
    "\n",
    "nbr_weeks = int((valid_start - train_start).days/7)\n",
    "# def prepare_dataset(df, promo_features, start_time, is_train=True):\n",
    "\n",
    "X_l, y_l = [], []\n",
    "\n",
    "for i in tqdm(range(nbr_weeks),mininterval = 300, desc = 'No. of week'):\n",
    "    delta = timedelta(days=7 * i)\n",
    "    X_tmp, y_tmp = gen_dataset(\n",
    "        df_train,\n",
    "        promo_features,\n",
    "        train_start + delta\n",
    "    )\n",
    "    X_l.append(X_tmp)\n",
    "    y_l.append(y_tmp)\n",
    "#     break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = pd.concat(X_l, axis=0)\n",
    "y_train = np.concatenate(y_l, axis=0)\n",
    "del X_l, y_l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "X_val, y_val = gen_dataset(df_train, promo_features, valid_start)\n",
    "X_test = gen_dataset(df_train, promo_features, test_start, is_train=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training and predicting models...\n"
     ]
    }
   ],
   "source": [
    "print(\"Training and predicting models...\")\n",
    "params = {\n",
    "    'num_leaves': 2**5 - 1,\n",
    "    'objective': 'regression_l2',\n",
    "    'max_depth': 8,\n",
    "    'min_data_in_leaf': 50,\n",
    "    'learning_rate': 0.05,\n",
    "    'feature_fraction': 0.75,\n",
    "    'bagging_fraction': 0.75,\n",
    "    'bagging_freq': 1,\n",
    "    'metric': 'l2',\n",
    "    'num_threads': 4\n",
    "}\n",
    "\n",
    "MAX_ROUNDS = 700\n",
    "val_pred = []\n",
    "test_pred = []\n",
    "cate_vars = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "  0%|          | 0/16 [00:00<?, ?it/s]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean_7_sales: 25761281.25\n",
      "mean_14_sales: 13565388.16\n",
      "mean_30_sales: 3805744.59\n",
      "lag_1_sales: 1973163.88\n",
      "promo_0: 1346309.19\n",
      "mean_60_sales: 833364.90\n",
      "mean_3_sales: 474000.88\n",
      "lag_2_sales: 303897.88\n",
      "lag_14_sales: 249816.62\n",
      "lag_28_sales: 221985.03\n",
      "sum_14_promo: 216708.68\n",
      "promo_7: 165361.07\n",
      "lag_35_sales: 127290.83\n",
      "lag_7_sales: 119902.43\n",
      "lag_21_sales: 79211.37\n",
      "lag_5_sales: 55032.23\n",
      "promo_3: 39604.49\n",
      "promo_14: 37187.13\n",
      "promo_15: 28214.24\n",
      "promo_1: 17566.00\n",
      "promo_9: 16382.88\n",
      "promo_6: 14197.75\n",
      "promo_5: 12357.71\n",
      "promo_4: 10795.03\n",
      "promo_2: 9617.96\n",
      "promo_11: 8151.04\n",
      "promo_13: 5040.33\n",
      "promo_10: 5036.19\n",
      "promo_8: 4335.00\n",
      "sum_60_promo: 2726.03\n",
      "promo_12: 1755.47\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "  6%|▋         | 1/16 [03:56<59:03, 236.23s/it]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean_7_sales: 19277636.84\n",
      "mean_14_sales: 14391371.01\n",
      "mean_30_sales: 3225710.07\n",
      "mean_60_sales: 1274420.77\n",
      "lag_1_sales: 898622.57\n",
      "promo_1: 841999.95\n",
      "mean_3_sales: 222263.29\n",
      "lag_2_sales: 171436.08\n",
      "lag_5_sales: 162252.85\n",
      "promo_0: 111229.47\n",
      "promo_3: 92581.60\n",
      "sum_14_promo: 73708.60\n",
      "lag_28_sales: 60954.24\n",
      "promo_5: 54927.08\n",
      "promo_7: 43946.19\n",
      "promo_2: 38461.55\n",
      "lag_7_sales: 37070.71\n",
      "promo_14: 24217.17\n",
      "promo_4: 23448.94\n",
      "lag_14_sales: 23015.61\n",
      "lag_35_sales: 21571.39\n",
      "lag_21_sales: 14102.23\n",
      "promo_8: 10089.50\n",
      "promo_6: 9886.49\n",
      "promo_15: 6674.41\n",
      "promo_9: 5026.78\n",
      "promo_11: 4635.79\n",
      "promo_10: 3048.42\n",
      "sum_60_promo: 2990.86\n",
      "promo_13: 2612.29\n",
      "promo_12: 1572.04\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      " 12%|█▎        | 2/16 [07:43<54:31, 233.66s/it]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean_14_sales: 19651037.58\n",
      "mean_7_sales: 17821638.92\n",
      "mean_30_sales: 3812610.11\n",
      "mean_60_sales: 1956586.76\n",
      "promo_2: 1336623.44\n",
      "lag_5_sales: 966007.26\n",
      "lag_1_sales: 323786.18\n",
      "mean_3_sales: 194325.22\n",
      "promo_9: 94903.01\n",
      "promo_3: 87090.37\n",
      "promo_7: 85218.06\n",
      "promo_0: 77387.23\n",
      "lag_28_sales: 75611.85\n",
      "sum_14_promo: 72698.58\n",
      "promo_5: 64287.43\n",
      "lag_2_sales: 54603.49\n",
      "promo_4: 53023.60\n",
      "lag_7_sales: 40711.55\n",
      "promo_1: 35619.97\n",
      "promo_10: 35427.62\n",
      "lag_35_sales: 27043.60\n",
      "promo_14: 26039.67\n",
      "promo_6: 25592.35\n",
      "lag_21_sales: 25284.68\n",
      "lag_14_sales: 22731.10\n",
      "promo_11: 20503.24\n",
      "promo_8: 14349.31\n",
      "promo_15: 12106.05\n",
      "promo_12: 5606.72\n",
      "sum_60_promo: 5605.16\n",
      "promo_13: 5521.77\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      " 19%|█▉        | 3/16 [11:36<50:34, 233.40s/it]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean_14_sales: 25000663.08\n",
      "mean_7_sales: 19468446.82\n",
      "mean_30_sales: 4572300.31\n",
      "mean_60_sales: 2430244.99\n",
      "promo_3: 849088.75\n",
      "mean_3_sales: 559860.80\n",
      "promo_7: 113541.53\n",
      "sum_14_promo: 93059.54\n",
      "lag_1_sales: 79594.81\n",
      "promo_0: 74762.45\n",
      "promo_5: 72944.76\n",
      "promo_1: 67763.40\n",
      "lag_7_sales: 66585.72\n",
      "lag_5_sales: 63855.54\n",
      "promo_2: 51656.27\n",
      "lag_2_sales: 48597.85\n",
      "promo_4: 42154.70\n",
      "promo_6: 37501.39\n",
      "promo_14: 28585.42\n",
      "lag_35_sales: 25624.98\n",
      "lag_28_sales: 24629.03\n",
      "lag_14_sales: 19189.21\n",
      "lag_21_sales: 18966.54\n",
      "promo_10: 14398.48\n",
      "promo_9: 10290.66\n",
      "promo_11: 7933.05\n",
      "sum_60_promo: 7003.81\n",
      "promo_13: 6815.80\n",
      "promo_8: 6676.34\n",
      "promo_15: 5177.09\n",
      "promo_12: 1808.85\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      " 25%|██▌       | 4/16 [15:35<47:01, 235.12s/it]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean_14_sales: 23215141.89\n",
      "mean_3_sales: 14514364.24\n",
      "mean_30_sales: 5268967.47\n",
      "mean_7_sales: 3925082.89\n",
      "mean_60_sales: 3817383.30\n",
      "promo_4: 1158013.74\n",
      "promo_3: 428163.48\n",
      "lag_2_sales: 303194.05\n",
      "lag_1_sales: 245510.08\n",
      "promo_1: 213217.89\n",
      "promo_7: 103501.64\n",
      "promo_0: 92702.24\n",
      "promo_2: 91862.09\n",
      "promo_5: 90915.71\n",
      "sum_14_promo: 69760.18\n",
      "promo_6: 53910.32\n",
      "lag_14_sales: 52828.45\n",
      "lag_5_sales: 48054.69\n",
      "promo_11: 40083.26\n",
      "lag_35_sales: 34623.44\n",
      "lag_28_sales: 34219.31\n",
      "promo_10: 32994.68\n",
      "lag_21_sales: 31468.45\n",
      "promo_14: 30432.94\n",
      "lag_7_sales: 24563.31\n",
      "promo_9: 19133.41\n",
      "promo_8: 16566.26\n",
      "promo_12: 7919.92\n",
      "promo_13: 7464.82\n",
      "sum_60_promo: 6078.18\n",
      "promo_15: 5569.76\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      " 31%|███▏      | 5/16 [19:49<44:06, 240.61s/it]\u001b[A\u001b[A"
     ]
    }
   ],
   "source": [
    "for i in tqdm(range(16)):\n",
    "    dtrain = lgb.Dataset(\n",
    "        X_train, label=y_train[:, i],\n",
    "        categorical_feature=cate_vars,\n",
    "        weight=pd.concat([items[\"perishable\"]] * nbr_weeks) * 0.25 + 1\n",
    "    )\n",
    "    dval = lgb.Dataset(\n",
    "        X_val, label=y_val[:, i], reference=dtrain,\n",
    "        weight=items[\"perishable\"] * 0.25 + 1,\n",
    "        categorical_feature=cate_vars)\n",
    "\n",
    "    bst = lgb.train(\n",
    "        params, dtrain, num_boost_round=MAX_ROUNDS, verbose_eval = False,\n",
    "        valid_sets=[dtrain, dval], early_stopping_rounds=50)\n",
    "    print(\"\\n\".join((\"%s: %.2f\" % x) for x in sorted(\n",
    "        zip(X_train.columns, bst.feature_importance(\"gain\")),\n",
    "        key=lambda x: x[1], reverse=True\n",
    "    )))\n",
    "    val_pred.append(bst.predict(\n",
    "        X_val, num_iteration=bst.best_iteration or MAX_ROUNDS))\n",
    "    test_pred.append(bst.predict(\n",
    "        X_test, num_iteration=bst.best_iteration or MAX_ROUNDS))\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generate submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation mse: 0.3594203168120458\n",
      "Making submission...\n"
     ]
    }
   ],
   "source": [
    "mse = mean_squared_error(y_val, np.array(val_pred).transpose())\n",
    "\n",
    "mlflow.set_experiment('grocery forecasting')\n",
    "with mlflow.start_run(run_name='lgbm'):\n",
    "    mlflow.log_param('model', 'lgbm')\n",
    "    mlflow.log_param('train starts', train_start)\n",
    "    mlflow.log_params(params)\n",
    "    mlflow.log_params('lagging': LAG_DICT.values)\n",
    "    mlflow.log_params('slidingWindows': SLIDING_DICT.values)\n",
    "    mlflow.log_metric('mse', mse)\n",
    "    \n",
    "print(\"Validation mse:\", mse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Making submission...\")\n",
    "y_test = np.array(test_pred).transpose()\n",
    "df_preds = pd.DataFrame(\n",
    "    y_test, index=df_train.index,\n",
    "    columns=pd.date_range(\"2017-08-16\", periods=16)\n",
    ").stack().to_frame(\"unit_sales\")\n",
    "df_preds.index.set_names([\"store_nbr\", \"item_nbr\", \"date\"], inplace=True)\n",
    "\n",
    "submission = df_test[[\"id\"]].join(df_preds, how=\"left\").fillna(0)\n",
    "submission[\"unit_sales\"] = np.clip(np.expm1(submission[\"unit_sales\"]), 0, 1000)\n",
    "submission.to_csv('lgb.csv', float_format='%.4f', index=None)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
