{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/liuyu/anaconda3/lib/python3.7/site-packages/lightgbm/__init__.py:46: UserWarning: Starting from version 2.2.1, the library file in distribution wheels for macOS is built by the Apple Clang (Xcode_8.3.3) compiler.\n",
      "This means that in case of installing LightGBM from PyPI via the ``pip install lightgbm`` command, you don't need to install the gcc compiler anymore.\n",
      "Instead of that, you need to install the OpenMP library, which is required for running LightGBM on the system with the Apple Clang compiler.\n",
      "You can install the OpenMP library by the following command: ``brew install libomp``.\n",
      "  \"You can install the OpenMP library by the following command: ``brew install libomp``.\", UserWarning)\n"
     ]
    }
   ],
   "source": [
    "from datetime import date, timedelta\n",
    "import datetime\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm, tnrange\n",
    "\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import lightgbm as lgb\n",
    "\n",
    "import mlflow\n",
    "import mlflow.sklearn\n",
    "\n",
    "from config import (\n",
    "    RAW_DATA_DIR,\n",
    "    FEATURE_DIR,\n",
    "    LAG_DICT,\n",
    "    SLIDING_DICT\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# solve lightgbm error on MAC\n",
    "import os\n",
    "os.environ['KMP_DUPLICATE_LIB_OK']='True'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data\n",
    "df_train = pd.read_csv(\n",
    "    RAW_DATA_DIR+'train.csv', usecols=[1, 2, 3, 4, 5],\n",
    "    dtype={'onpromotion': bool},\n",
    "    converters={'unit_sales': lambda u: np.log1p(\n",
    "        float(u)) if float(u) > 0 else 0},\n",
    "    parse_dates=[\"date\"],\n",
    "    skiprows=range(1, 66458909)  # 2016-01-01\n",
    ")\n",
    "\n",
    "df_test = pd.read_csv(\n",
    "    RAW_DATA_DIR+'test.csv', usecols=[0, 1, 2, 3, 4],\n",
    "    dtype={'onpromotion': bool},\n",
    "    parse_dates=[\"date\"]  # , date_parser=parser\n",
    ").set_index(\n",
    "    ['store_nbr', 'item_nbr', 'date']\n",
    ")\n",
    "\n",
    "items = pd.read_csv(\n",
    "    RAW_DATA_DIR+'items.csv',\n",
    ").set_index(\"item_nbr\")\n",
    "\n",
    "stores = pd.read_csv(\n",
    "    RAW_DATA_DIR+'stores.csv',\n",
    ").set_index(\"store_nbr\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test Period\n",
    "\n",
    "2017-08-16 to 2017-08-31"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_start = date(2017, 8, 16)\n",
    "test_end = date(2017,8, 31)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "valid starts from 2017-07-26 to 2017-08-10\n"
     ]
    }
   ],
   "source": [
    "valid_start = test_start - timedelta(16)\n",
    "while(1):\n",
    "    if valid_start.weekday() == test_start.weekday():\n",
    "        break\n",
    "    valid_start = valid_start-timedelta(days=1)\n",
    "valid_end = valid_start + timedelta(15)\n",
    "print('valid starts from {} to {}'.format(valid_start, valid_end))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Valid Period\n",
    "\n",
    "Considering the more nearer peiods of sales data may have more in common, it would be better to find the nearest period as valid period.\n",
    "\n",
    "Based on the analysis before, we assume the sales data is periodically with the frequency of 7 days, so we want to keep that feature same\n",
    "in the train, valid and test period.\n",
    "\n",
    "So finally, we choose valid period:\n",
    "\n",
    "2017-07-26 to 2017-08-10\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_start = date(2017, 7, 26)\n",
    "valid_end = date(2017, 8, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filter Period\n",
    "\n",
    "#### Earthquake happended on April 16, 2016. It may affect for the next several weeks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train datasets starts from 2016-10-05\n"
     ]
    }
   ],
   "source": [
    "# filter the period which is affected by earthquake.\n",
    "filter_date = date(2016,4,16) + timedelta(7*4)\n",
    "lag_max = 140\n",
    "train_start=  filter_date+timedelta(days=lag_max)\n",
    "\n",
    "while(1):\n",
    "    train_start = train_start + timedelta(1)\n",
    "    if train_start.weekday() == valid_start.weekday():\n",
    "        break\n",
    "print('train datasets starts from {}'.format(train_start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_start = date(2017, 4, 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Wages in the public sector are paid every two weeks on the 15 th and on the last day of the month. Supermarket sales could be affected by this.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/liuyu/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:1: FutureWarning: Comparing Series of datetimes with 'datetime.date'.  Currently, the\n",
      "'datetime.date' is coerced to a datetime. In the future pandas will\n",
      "not coerce, and a TypeError will be raised. To retain the current\n",
      "behavior, convert the 'datetime.date' to a datetime with\n",
      "'pd.Timestamp'.\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    }
   ],
   "source": [
    "df_train = df_train[df_train['date']>=filter_date]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Promo feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "promo_train = df_train.set_index(\n",
    "    [\"store_nbr\", \"item_nbr\", \"date\"])[[\"onpromotion\"]]\n",
    "\n",
    "# missing onpromotions filling\n",
    "promo_train = promo_train.unstack(level=-1).fillna(False)\n",
    "promo_train.columns = promo_train.columns.get_level_values(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# missing test onpromotions filling\n",
    "promo_test = df_test[[\"onpromotion\"]].unstack(level=-1).fillna(False)\n",
    "promo_test.columns = promo_test.columns.get_level_values(1)\n",
    "# filter those items/stores in promo_test but not in promo_train\n",
    "promo_test = promo_test.reindex(promo_train.index).fillna(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "promo_features = pd.concat([promo_train, promo_test], axis=1)\n",
    "del promo_test, promo_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# label\n",
    "df_train = df_train.set_index([\"store_nbr\", \"item_nbr\", \"date\"])[[\"unit_sales\"]].unstack(level=-1).fillna(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Item"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "items = items.reindex(df_train.index.get_level_values(1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Item Family"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "items['family'] = items['family'].astype('category')\n",
    "item_family_features = items.family.cat.codes.values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Item's class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "items['class'] = items['class'].astype('category')\n",
    "item_class_features = items['class'].cat.codes.values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "stores = stores.reindex(df_train.index.get_level_values(0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Store's city"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "stores['city'] = stores['city'].astype('category')\n",
    "store_city_features = stores['city'].cat.codes.values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Store's state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "stores['state'] = stores['state'].astype('category')\n",
    "store_state_features = stores['state'].cat.codes.values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Store's type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "stores['type'] = stores['type'].astype('category')\n",
    "store_type_features = stores['type'].cat.codes.values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Store's cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "stores['cluster'] = stores['cluster'].astype('category')\n",
    "store_cluster_features = stores['cluster'].cat.codes.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.columns = df_train.columns.get_level_values(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Filling missing date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2016-12-25 00:00:00\n"
     ]
    }
   ],
   "source": [
    "date_list = df_train.columns\n",
    "obj_list = pd.date_range(filter_date, test_start-timedelta(1))\n",
    "diff_list = list(set(obj_list) - set(date_list)) \n",
    "for i in diff_list:\n",
    "    print(i)\n",
    "    df_train[i] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2016-12-25 00:00:00\n"
     ]
    }
   ],
   "source": [
    "date_list = promo_features.columns\n",
    "obj_list = pd.date_range(filter_date, test_end)\n",
    "diff_list = list(set(obj_list) - set(date_list)) \n",
    "for i in diff_list:\n",
    "    print(i)\n",
    "    promo_features[i] = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Lagging and sliding windows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "LAG_DICT = {'unit_sales': [1,2,3, 4, 5, 6, 7, 8, 9, 10 ,11 ,12, 13 ,14, 15, 16, 21,30, 60],\n",
    "            'onpromotion': [2, 3,4,5,6, 7, 14, 21]}\n",
    "\n",
    "SLIDING_DICT = {'unit_sales': [3, 4, 5, 6, 7, 14, 21, 30, 60]}\n",
    "\n",
    "# initialise dirs\n",
    "RAW_DATA_DIR = 'datasets/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "    ###多于NN模型的特征模块：\n",
    "【时间窗】\n",
    "    for i in [3, 7, 14, 30, 60, 140]:\n",
    "        tmp1 = get_timespan(df, t2017, i, i)\n",
    "        tmp2 = (get_timespan(promo_df, t2017, i, i) > 0) * 1\n",
    "【特征】促销日的销量均值\n",
    "        X['has_promo_mean_%s' % i] = (tmp1 * tmp2.replace(0, np.nan)).mean(axis=1).values\n",
    "【特征】促销日的销量递增加权和。np.power(0.9, np.arange(i)[::-1])产生指数递增权重\n",
    "        X['has_promo_mean_%s_decay' % i] = (tmp1 * tmp2.replace(0, np.nan) * np.power(0.9, np.arange(i)[::-1])).sum(axis=1).values\n",
    "【特征】非促销日\n",
    "        X['no_promo_mean_%s' % i] = (tmp1 * (1 - tmp2).replace(0, np.nan)).mean(axis=1).values\n",
    "        X['no_promo_mean_%s_decay' % i] = (tmp1 * (1 - tmp2).replace(0, np.nan) * np.power(0.9, np.arange(i)[::-1])).sum(axis=1).values\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "datetime.date(2017, 4, 21)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_start + timedelta(days=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_timespan(df, \n",
    "                 start_time,\n",
    "                 minus,\n",
    "                 periods,\n",
    "                 freq='D'):\n",
    "    return df[pd.date_range(start_time - timedelta(days=minus), periods=periods, freq=freq)]\n",
    "\n",
    "def gen_dataset(df, \n",
    "                promo_features,\n",
    "                item_family_features,\n",
    "                item_class_features,\n",
    "                store_city_features,\n",
    "                store_state_features,\n",
    "                store_type_features,\n",
    "                store_cluster_features,\n",
    "                start_time,\n",
    "                is_train=True):\n",
    "    # init\n",
    "    X = pd.DataFrame()\n",
    "    \n",
    "    for i in LAG_DICT['unit_sales']:\n",
    "        X['lag_{}_sales'.format(i)] = get_timespan(df, start_time, i, 1).values.ravel()\n",
    "    \n",
    "    for i in LAG_DICT['onpromotion']:\n",
    "        X['sum_{}_promo'.format(i)] = get_timespan(promo_features, start_time, i, 1).sum(axis=1).ravel()\n",
    "        \n",
    "    for i in range(16):\n",
    "        X['sum_{}_promo_test'.format(i)]= get_timespan(promo_features, start_time + timedelta(days=16), 15, i).sum(axis=1).values\n",
    "        \n",
    "    for i in SLIDING_DICT['unit_sales']:\n",
    "        X[\"mean_{}_sales\".format(i)] = get_timespan(df, start_time, i, i).mean(axis=1).values\n",
    "        X[\"std_{}_sales\".format(i)] = get_timespan(df, start_time, i, i).std(axis=1).values\n",
    "        X[\"min_{}_sales\".format(i)] = get_timespan(df, start_time, i, i).min(axis=1).values\n",
    "        X[\"max_{}_sales\".format(i)] = get_timespan(df, start_time, i, i).max(axis=1).values\n",
    "        X[\"median_{}_sales\".format(i)] = get_timespan(df, start_time, i, i).median(axis=1).values\n",
    "\n",
    "\n",
    "    for i in range(7):\n",
    "        X['mean_4_dow{}_2017'.format(i)] = get_timespan(df, start_time, 28-i, 4, freq='7D').mean(axis=1).values\n",
    "        X['mean_20_dow{}_2017'.format(i)] = get_timespan(df, start_time, 140-i, 20, freq='7D').mean(axis=1).values\n",
    "        \n",
    "    # for the next to-predict 16 days \n",
    "    for i in range(16):\n",
    "        X[\"promo_{}\".format(i)] = promo_features[start_time + timedelta(days=i)].values.astype(np.uint8)\n",
    "\n",
    "    for i in [7, 14, 30, 60, 140]:\n",
    "        tmp = get_timespan(df, start_time, i, i)\n",
    "\n",
    "        X['has_sales_days_in_last_{}'.format(i)] = (tmp > 0).sum(axis=1).values\n",
    "        X['last_has_sales_day_in_last_%s' % i] = i - ((tmp > 0) * np.arange(i)).max(axis=1).values\n",
    "\n",
    "        tmp = get_timespan(promo_features, start_time, i, i)\n",
    "        X['has_promo_days_in_last_%s' % i] = (tmp > 0).sum(axis=1).values\n",
    "        X['last_has_promo_day_in_last_%s' % i] = i - ((tmp > 0) * np.arange(i)).max(axis=1).values\n",
    "\n",
    "\n",
    "        \n",
    "    X['item_family_features'] = item_family_features\n",
    "\n",
    "    X['item_class_features'] = item_class_features\n",
    "\n",
    "    X['store_city_features'] = store_city_features\n",
    "\n",
    "    X['store_state_features'] = store_state_features\n",
    "\n",
    "    X['store_type_features'] = store_type_features\n",
    "\n",
    "    X['store_cluster_features'] = store_cluster_features\n",
    "        \n",
    "    if is_train:\n",
    "        y = df[pd.date_range(start_time, periods=16)].values\n",
    "        return X, y\n",
    "    return X\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generate train, valid and test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No. of week:   0%|          | 0/16 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing dataset...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No. of week: 100%|██████████| 16/16 [01:30<00:00,  5.71s/it]\n"
     ]
    }
   ],
   "source": [
    "print(\"Preparing dataset...\")\n",
    "\n",
    "nbr_weeks = int((valid_start - train_start).days/7)\n",
    "\n",
    "X_l, y_l = [], []\n",
    "\n",
    "for i in tqdm(range(nbr_weeks), desc = 'No. of week'):\n",
    "    delta = timedelta(days=7 * i)\n",
    "    X_tmp, y_tmp = gen_dataset(\n",
    "        df_train,\n",
    "        promo_features,\n",
    "        item_family_features,\n",
    "        item_class_features,\n",
    "        store_city_features,\n",
    "        store_state_features,\n",
    "        store_type_features,\n",
    "        store_cluster_features,\n",
    "        train_start + delta\n",
    "    )\n",
    "    X_l.append(X_tmp)\n",
    "    y_l.append(y_tmp)\n",
    "#     break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = pd.concat(X_l, axis=0)\n",
    "y_train = np.concatenate(y_l, axis=0)\n",
    "del X_l, y_l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "X_val, y_val = gen_dataset(df_train,\n",
    "                           promo_features,\n",
    "                           item_family_features,\n",
    "                           item_class_features,\n",
    "                           store_city_features,\n",
    "                           store_state_features,\n",
    "                           store_type_features,\n",
    "                           store_cluster_features,\n",
    "                           valid_start)\n",
    "X_test = gen_dataset(df_train, \n",
    "                    promo_features,\n",
    "                    item_family_features,\n",
    "                    item_class_features,\n",
    "                    store_city_features,\n",
    "                    store_state_features,\n",
    "                    store_type_features,\n",
    "                    store_cluster_features,\n",
    "                    test_start, is_train=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training and predicting models...\n"
     ]
    }
   ],
   "source": [
    "print(\"Training and predicting models...\")\n",
    "params = {\n",
    "    'num_leaves': 2**8 - 1,\n",
    "    'objective': 'regression_l2',\n",
    "    'max_depth': 8,\n",
    "    'min_data_in_leaf': 50,\n",
    "    'learning_rate': 0.05,\n",
    "    'feature_fraction': 0.75,\n",
    "    'bagging_fraction': 0.75,\n",
    "    'bagging_freq': 1,\n",
    "    'metric': 'l2',\n",
    "    'num_threads': 4\n",
    "}\n",
    "\n",
    "MAX_ROUNDS = 200\n",
    "val_pred = []\n",
    "test_pred = []\n",
    "cate_vars = ['item_family_features',\n",
    "            'item_class_features',\n",
    "            'store_city_features',\n",
    "            'store_state_features',\n",
    "            'store_type_features',\n",
    "            'store_cluster_features']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/16 [00:00<?, ?it/s]/Users/liuyu/anaconda3/lib/python3.7/site-packages/lightgbm/basic.py:1205: UserWarning: Using categorical_feature in Dataset.\n",
      "  warnings.warn('Using categorical_feature in Dataset.')\n",
      "/Users/liuyu/anaconda3/lib/python3.7/site-packages/lightgbm/basic.py:762: UserWarning: categorical_feature in param dict is overridden.\n",
      "  warnings.warn('categorical_feature in param dict is overridden.')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1]\ttraining's l2: 1.04308\tvalid_1's l2: 1.00166\n",
      "Training until validation scores don't improve for 50 rounds.\n",
      "[2]\ttraining's l2: 0.971876\tvalid_1's l2: 0.932673\n",
      "[3]\ttraining's l2: 0.909421\tvalid_1's l2: 0.872389\n",
      "[4]\ttraining's l2: 0.851101\tvalid_1's l2: 0.816019\n",
      "[5]\ttraining's l2: 0.798314\tvalid_1's l2: 0.764964\n",
      "[6]\ttraining's l2: 0.750631\tvalid_1's l2: 0.718945\n",
      "[7]\ttraining's l2: 0.707583\tvalid_1's l2: 0.677254\n",
      "[8]\ttraining's l2: 0.668575\tvalid_1's l2: 0.639617\n",
      "[9]\ttraining's l2: 0.634347\tvalid_1's l2: 0.606684\n",
      "[10]\ttraining's l2: 0.602286\tvalid_1's l2: 0.575777\n",
      "[11]\ttraining's l2: 0.573257\tvalid_1's l2: 0.5478\n",
      "[12]\ttraining's l2: 0.546986\tvalid_1's l2: 0.522542\n",
      "[13]\ttraining's l2: 0.523325\tvalid_1's l2: 0.499736\n",
      "[14]\ttraining's l2: 0.501798\tvalid_1's l2: 0.479107\n",
      "[15]\ttraining's l2: 0.482315\tvalid_1's l2: 0.460431\n",
      "[16]\ttraining's l2: 0.465276\tvalid_1's l2: 0.444262\n",
      "[17]\ttraining's l2: 0.449904\tvalid_1's l2: 0.429631\n",
      "[18]\ttraining's l2: 0.435273\tvalid_1's l2: 0.415667\n",
      "[19]\ttraining's l2: 0.422045\tvalid_1's l2: 0.403105\n",
      "[20]\ttraining's l2: 0.410045\tvalid_1's l2: 0.391755\n",
      "[21]\ttraining's l2: 0.39914\tvalid_1's l2: 0.38137\n",
      "[22]\ttraining's l2: 0.389235\tvalid_1's l2: 0.37199\n",
      "[23]\ttraining's l2: 0.380662\tvalid_1's l2: 0.363935\n",
      "[24]\ttraining's l2: 0.37254\tvalid_1's l2: 0.356253\n",
      "[25]\ttraining's l2: 0.365093\tvalid_1's l2: 0.349225\n",
      "[26]\ttraining's l2: 0.358354\tvalid_1's l2: 0.342913\n",
      "[27]\ttraining's l2: 0.352519\tvalid_1's l2: 0.337509\n",
      "[28]\ttraining's l2: 0.346899\tvalid_1's l2: 0.332261\n",
      "[29]\ttraining's l2: 0.341806\tvalid_1's l2: 0.327501\n",
      "[30]\ttraining's l2: 0.33716\tvalid_1's l2: 0.323182\n",
      "[31]\ttraining's l2: 0.332961\tvalid_1's l2: 0.319308\n",
      "[32]\ttraining's l2: 0.329332\tvalid_1's l2: 0.315984\n",
      "[33]\ttraining's l2: 0.326044\tvalid_1's l2: 0.313015\n",
      "[34]\ttraining's l2: 0.322839\tvalid_1's l2: 0.310087\n",
      "[35]\ttraining's l2: 0.32014\tvalid_1's l2: 0.307625\n",
      "[36]\ttraining's l2: 0.317699\tvalid_1's l2: 0.305417\n",
      "[37]\ttraining's l2: 0.315442\tvalid_1's l2: 0.303433\n",
      "[38]\ttraining's l2: 0.313098\tvalid_1's l2: 0.301289\n",
      "[39]\ttraining's l2: 0.31098\tvalid_1's l2: 0.299367\n",
      "[40]\ttraining's l2: 0.309049\tvalid_1's l2: 0.297609\n",
      "[41]\ttraining's l2: 0.307268\tvalid_1's l2: 0.296009\n",
      "[42]\ttraining's l2: 0.305701\tvalid_1's l2: 0.294632\n",
      "[43]\ttraining's l2: 0.304214\tvalid_1's l2: 0.293332\n",
      "[44]\ttraining's l2: 0.302845\tvalid_1's l2: 0.292136\n",
      "[45]\ttraining's l2: 0.301546\tvalid_1's l2: 0.291006\n",
      "[46]\ttraining's l2: 0.300393\tvalid_1's l2: 0.28997\n",
      "[47]\ttraining's l2: 0.299322\tvalid_1's l2: 0.289054\n",
      "[48]\ttraining's l2: 0.298379\tvalid_1's l2: 0.288299\n",
      "[49]\ttraining's l2: 0.297474\tvalid_1's l2: 0.28757\n",
      "[50]\ttraining's l2: 0.296662\tvalid_1's l2: 0.286886\n",
      "[51]\ttraining's l2: 0.295893\tvalid_1's l2: 0.286237\n",
      "[52]\ttraining's l2: 0.295177\tvalid_1's l2: 0.28563\n",
      "[53]\ttraining's l2: 0.294595\tvalid_1's l2: 0.285171\n",
      "[54]\ttraining's l2: 0.293989\tvalid_1's l2: 0.284677\n",
      "[55]\ttraining's l2: 0.293418\tvalid_1's l2: 0.284241\n",
      "[56]\ttraining's l2: 0.292854\tvalid_1's l2: 0.28378\n",
      "[57]\ttraining's l2: 0.292365\tvalid_1's l2: 0.283407\n",
      "[58]\ttraining's l2: 0.29191\tvalid_1's l2: 0.283044\n",
      "[59]\ttraining's l2: 0.291508\tvalid_1's l2: 0.282747\n",
      "[60]\ttraining's l2: 0.291134\tvalid_1's l2: 0.282503\n",
      "[61]\ttraining's l2: 0.290762\tvalid_1's l2: 0.28223\n",
      "[62]\ttraining's l2: 0.290409\tvalid_1's l2: 0.281981\n",
      "[63]\ttraining's l2: 0.290028\tvalid_1's l2: 0.281721\n",
      "[64]\ttraining's l2: 0.289714\tvalid_1's l2: 0.281497\n",
      "[65]\ttraining's l2: 0.289412\tvalid_1's l2: 0.281287\n",
      "[66]\ttraining's l2: 0.28911\tvalid_1's l2: 0.281094\n",
      "[67]\ttraining's l2: 0.288827\tvalid_1's l2: 0.28088\n",
      "[68]\ttraining's l2: 0.288574\tvalid_1's l2: 0.28072\n",
      "[69]\ttraining's l2: 0.288365\tvalid_1's l2: 0.280601\n",
      "[70]\ttraining's l2: 0.288131\tvalid_1's l2: 0.280431\n",
      "[71]\ttraining's l2: 0.287931\tvalid_1's l2: 0.280313\n",
      "[72]\ttraining's l2: 0.287762\tvalid_1's l2: 0.280196\n",
      "[73]\ttraining's l2: 0.287533\tvalid_1's l2: 0.280059\n",
      "[74]\ttraining's l2: 0.287326\tvalid_1's l2: 0.279932\n",
      "[75]\ttraining's l2: 0.287098\tvalid_1's l2: 0.279817\n",
      "[76]\ttraining's l2: 0.286916\tvalid_1's l2: 0.279723\n",
      "[77]\ttraining's l2: 0.286746\tvalid_1's l2: 0.279646\n",
      "[78]\ttraining's l2: 0.286619\tvalid_1's l2: 0.279598\n",
      "[79]\ttraining's l2: 0.286476\tvalid_1's l2: 0.279523\n",
      "[80]\ttraining's l2: 0.286295\tvalid_1's l2: 0.279401\n",
      "[81]\ttraining's l2: 0.286118\tvalid_1's l2: 0.279313\n",
      "[82]\ttraining's l2: 0.285981\tvalid_1's l2: 0.279257\n",
      "[83]\ttraining's l2: 0.285858\tvalid_1's l2: 0.279214\n",
      "[84]\ttraining's l2: 0.285728\tvalid_1's l2: 0.279167\n",
      "[85]\ttraining's l2: 0.285606\tvalid_1's l2: 0.279114\n",
      "[86]\ttraining's l2: 0.285466\tvalid_1's l2: 0.279072\n",
      "[87]\ttraining's l2: 0.285372\tvalid_1's l2: 0.279004\n",
      "[88]\ttraining's l2: 0.285254\tvalid_1's l2: 0.278959\n",
      "[89]\ttraining's l2: 0.285124\tvalid_1's l2: 0.278901\n",
      "[90]\ttraining's l2: 0.284973\tvalid_1's l2: 0.278792\n",
      "[91]\ttraining's l2: 0.284875\tvalid_1's l2: 0.278739\n",
      "[92]\ttraining's l2: 0.284739\tvalid_1's l2: 0.27868\n",
      "[93]\ttraining's l2: 0.284666\tvalid_1's l2: 0.278657\n",
      "[94]\ttraining's l2: 0.284552\tvalid_1's l2: 0.278617\n",
      "[95]\ttraining's l2: 0.28445\tvalid_1's l2: 0.278563\n",
      "[96]\ttraining's l2: 0.28436\tvalid_1's l2: 0.278542\n",
      "[97]\ttraining's l2: 0.284272\tvalid_1's l2: 0.278532\n",
      "[98]\ttraining's l2: 0.284187\tvalid_1's l2: 0.278509\n",
      "[99]\ttraining's l2: 0.2841\tvalid_1's l2: 0.278488\n",
      "[100]\ttraining's l2: 0.283999\tvalid_1's l2: 0.278427\n",
      "[101]\ttraining's l2: 0.28391\tvalid_1's l2: 0.278413\n",
      "[102]\ttraining's l2: 0.283822\tvalid_1's l2: 0.278365\n",
      "[103]\ttraining's l2: 0.283733\tvalid_1's l2: 0.278334\n",
      "[104]\ttraining's l2: 0.28363\tvalid_1's l2: 0.27828\n",
      "[105]\ttraining's l2: 0.283547\tvalid_1's l2: 0.278254\n",
      "[106]\ttraining's l2: 0.283472\tvalid_1's l2: 0.278223\n",
      "[107]\ttraining's l2: 0.283398\tvalid_1's l2: 0.278205\n",
      "[108]\ttraining's l2: 0.283328\tvalid_1's l2: 0.278191\n",
      "[109]\ttraining's l2: 0.283213\tvalid_1's l2: 0.278132\n",
      "[110]\ttraining's l2: 0.283129\tvalid_1's l2: 0.2781\n",
      "[111]\ttraining's l2: 0.283064\tvalid_1's l2: 0.278083\n",
      "[112]\ttraining's l2: 0.282991\tvalid_1's l2: 0.278064\n",
      "[113]\ttraining's l2: 0.282892\tvalid_1's l2: 0.27801\n",
      "[114]\ttraining's l2: 0.282814\tvalid_1's l2: 0.277974\n",
      "[115]\ttraining's l2: 0.282742\tvalid_1's l2: 0.277957\n",
      "[116]\ttraining's l2: 0.282637\tvalid_1's l2: 0.277918\n",
      "[117]\ttraining's l2: 0.282537\tvalid_1's l2: 0.277869\n",
      "[118]\ttraining's l2: 0.282432\tvalid_1's l2: 0.277855\n",
      "[119]\ttraining's l2: 0.282377\tvalid_1's l2: 0.277855\n",
      "[120]\ttraining's l2: 0.282303\tvalid_1's l2: 0.277816\n",
      "[121]\ttraining's l2: 0.282223\tvalid_1's l2: 0.277836\n",
      "[122]\ttraining's l2: 0.282161\tvalid_1's l2: 0.277816\n",
      "[123]\ttraining's l2: 0.282119\tvalid_1's l2: 0.277802\n",
      "[124]\ttraining's l2: 0.282036\tvalid_1's l2: 0.277762\n",
      "[125]\ttraining's l2: 0.281978\tvalid_1's l2: 0.277743\n",
      "[126]\ttraining's l2: 0.281913\tvalid_1's l2: 0.277703\n",
      "[127]\ttraining's l2: 0.281845\tvalid_1's l2: 0.27769\n",
      "[128]\ttraining's l2: 0.281787\tvalid_1's l2: 0.277673\n",
      "[129]\ttraining's l2: 0.281738\tvalid_1's l2: 0.277685\n",
      "[130]\ttraining's l2: 0.28167\tvalid_1's l2: 0.277666\n",
      "[131]\ttraining's l2: 0.281624\tvalid_1's l2: 0.277639\n",
      "[132]\ttraining's l2: 0.281578\tvalid_1's l2: 0.277632\n",
      "[133]\ttraining's l2: 0.281529\tvalid_1's l2: 0.277618\n",
      "[134]\ttraining's l2: 0.281454\tvalid_1's l2: 0.277584\n",
      "[135]\ttraining's l2: 0.281406\tvalid_1's l2: 0.277575\n",
      "[136]\ttraining's l2: 0.28135\tvalid_1's l2: 0.277572\n",
      "[137]\ttraining's l2: 0.281296\tvalid_1's l2: 0.277564\n",
      "[138]\ttraining's l2: 0.281212\tvalid_1's l2: 0.277527\n",
      "[139]\ttraining's l2: 0.28115\tvalid_1's l2: 0.277507\n",
      "[140]\ttraining's l2: 0.281095\tvalid_1's l2: 0.277495\n",
      "[141]\ttraining's l2: 0.281036\tvalid_1's l2: 0.277485\n",
      "[142]\ttraining's l2: 0.280976\tvalid_1's l2: 0.277476\n",
      "[143]\ttraining's l2: 0.280938\tvalid_1's l2: 0.277467\n",
      "[144]\ttraining's l2: 0.280889\tvalid_1's l2: 0.277465\n",
      "[145]\ttraining's l2: 0.280836\tvalid_1's l2: 0.277443\n",
      "[146]\ttraining's l2: 0.280783\tvalid_1's l2: 0.277441\n",
      "[147]\ttraining's l2: 0.280724\tvalid_1's l2: 0.277415\n",
      "[148]\ttraining's l2: 0.280677\tvalid_1's l2: 0.277433\n",
      "[149]\ttraining's l2: 0.280622\tvalid_1's l2: 0.277421\n",
      "[150]\ttraining's l2: 0.280568\tvalid_1's l2: 0.277405\n",
      "[151]\ttraining's l2: 0.28051\tvalid_1's l2: 0.277375\n",
      "[152]\ttraining's l2: 0.280463\tvalid_1's l2: 0.277357\n",
      "[153]\ttraining's l2: 0.280404\tvalid_1's l2: 0.277354\n",
      "[154]\ttraining's l2: 0.280356\tvalid_1's l2: 0.27734\n",
      "[155]\ttraining's l2: 0.280312\tvalid_1's l2: 0.277329\n",
      "[156]\ttraining's l2: 0.280261\tvalid_1's l2: 0.277324\n",
      "[157]\ttraining's l2: 0.2802\tvalid_1's l2: 0.277317\n",
      "[158]\ttraining's l2: 0.280138\tvalid_1's l2: 0.277322\n",
      "[159]\ttraining's l2: 0.2801\tvalid_1's l2: 0.277312\n",
      "[160]\ttraining's l2: 0.280061\tvalid_1's l2: 0.277304\n",
      "[161]\ttraining's l2: 0.280022\tvalid_1's l2: 0.27729\n",
      "[162]\ttraining's l2: 0.279965\tvalid_1's l2: 0.277287\n",
      "[163]\ttraining's l2: 0.279907\tvalid_1's l2: 0.277278\n",
      "[164]\ttraining's l2: 0.279856\tvalid_1's l2: 0.277274\n",
      "[165]\ttraining's l2: 0.279803\tvalid_1's l2: 0.277271\n",
      "[166]\ttraining's l2: 0.279753\tvalid_1's l2: 0.277261\n",
      "[167]\ttraining's l2: 0.279712\tvalid_1's l2: 0.277246\n",
      "[168]\ttraining's l2: 0.279655\tvalid_1's l2: 0.277244\n",
      "[169]\ttraining's l2: 0.279611\tvalid_1's l2: 0.277226\n",
      "[170]\ttraining's l2: 0.279567\tvalid_1's l2: 0.277225\n",
      "[171]\ttraining's l2: 0.279513\tvalid_1's l2: 0.277212\n",
      "[172]\ttraining's l2: 0.279465\tvalid_1's l2: 0.277224\n",
      "[173]\ttraining's l2: 0.279434\tvalid_1's l2: 0.277216\n",
      "[174]\ttraining's l2: 0.279393\tvalid_1's l2: 0.277205\n",
      "[175]\ttraining's l2: 0.27935\tvalid_1's l2: 0.277197\n",
      "[176]\ttraining's l2: 0.279298\tvalid_1's l2: 0.277197\n",
      "[177]\ttraining's l2: 0.279259\tvalid_1's l2: 0.277198\n",
      "[178]\ttraining's l2: 0.279209\tvalid_1's l2: 0.277186\n",
      "[179]\ttraining's l2: 0.279159\tvalid_1's l2: 0.277181\n",
      "[180]\ttraining's l2: 0.279113\tvalid_1's l2: 0.277174\n",
      "[181]\ttraining's l2: 0.279075\tvalid_1's l2: 0.277169\n",
      "[182]\ttraining's l2: 0.279035\tvalid_1's l2: 0.277167\n",
      "[183]\ttraining's l2: 0.278993\tvalid_1's l2: 0.277154\n",
      "[184]\ttraining's l2: 0.278947\tvalid_1's l2: 0.27715\n",
      "[185]\ttraining's l2: 0.278915\tvalid_1's l2: 0.277144\n",
      "[186]\ttraining's l2: 0.278892\tvalid_1's l2: 0.277139\n",
      "[187]\ttraining's l2: 0.278844\tvalid_1's l2: 0.277142\n",
      "[188]\ttraining's l2: 0.278807\tvalid_1's l2: 0.277145\n",
      "[189]\ttraining's l2: 0.27877\tvalid_1's l2: 0.277139\n",
      "[190]\ttraining's l2: 0.27872\tvalid_1's l2: 0.277122\n",
      "[191]\ttraining's l2: 0.278662\tvalid_1's l2: 0.277119\n",
      "[192]\ttraining's l2: 0.278621\tvalid_1's l2: 0.277107\n",
      "[193]\ttraining's l2: 0.27858\tvalid_1's l2: 0.277098\n",
      "[194]\ttraining's l2: 0.278543\tvalid_1's l2: 0.277092\n",
      "[195]\ttraining's l2: 0.278509\tvalid_1's l2: 0.277092\n",
      "[196]\ttraining's l2: 0.278459\tvalid_1's l2: 0.277077\n",
      "[197]\ttraining's l2: 0.278421\tvalid_1's l2: 0.277066\n",
      "[198]\ttraining's l2: 0.278386\tvalid_1's l2: 0.27706\n",
      "[199]\ttraining's l2: 0.278342\tvalid_1's l2: 0.277056\n",
      "[200]\ttraining's l2: 0.278308\tvalid_1's l2: 0.277049\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[200]\ttraining's l2: 0.278308\tvalid_1's l2: 0.277049\n",
      "mean_7_sales: 10081967.69\n",
      "mean_14_sales: 5375676.97\n",
      "promo_0: 498011.03\n",
      "mean_20_dow0_2017: 360062.31\n",
      "mean_5_sales: 262553.46\n",
      "item_class_features: 197508.28\n",
      "lag_1_sales: 182191.82\n",
      "mean_6_sales: 176170.44\n",
      "mean_4_dow0_2017: 148878.49\n",
      "mean_30_sales: 146969.66\n",
      "mean_21_sales: 132387.52\n",
      "median_7_sales: 97627.01\n",
      "mean_60_sales: 87597.39\n",
      "has_promo_days_in_last_7: 85035.82\n",
      "sum_7_promo: 79328.68\n",
      "last_has_sales_day_in_last_7: 64158.99\n",
      "std_14_sales: 49532.08\n",
      "median_30_sales: 48644.47\n",
      "store_cluster_features: 47539.65\n",
      "std_7_sales: 42131.43\n",
      "median_14_sales: 40887.95\n",
      "item_family_features: 33850.01\n",
      "mean_3_sales: 25976.24\n",
      "mean_4_dow6_2017: 23676.05\n",
      "median_3_sales: 22977.80\n",
      "last_has_sales_day_in_last_14: 21032.84\n",
      "promo_7: 19573.85\n",
      "sum_14_promo: 19178.78\n",
      "lag_2_sales: 17565.10\n",
      "std_30_sales: 17063.69\n",
      "max_5_sales: 16716.78\n",
      "std_60_sales: 16005.39\n",
      "has_sales_days_in_last_140: 15676.47\n",
      "std_21_sales: 15355.14\n",
      "store_city_features: 13344.91\n",
      "median_60_sales: 13228.52\n",
      "max_3_sales: 13176.48\n",
      "lag_14_sales: 11387.58\n",
      "has_promo_days_in_last_14: 10317.87\n",
      "store_type_features: 10052.27\n",
      "mean_4_sales: 9862.01\n",
      "median_6_sales: 9542.59\n",
      "has_promo_days_in_last_30: 9339.10\n",
      "max_14_sales: 9238.96\n",
      "mean_20_dow1_2017: 8678.85\n",
      "min_3_sales: 7842.45\n",
      "sum_6_promo_test: 7562.41\n",
      "last_has_promo_day_in_last_7: 7398.57\n",
      "std_5_sales: 7165.31\n",
      "lag_7_sales: 7154.80\n",
      "mean_20_dow6_2017: 6800.24\n",
      "has_promo_days_in_last_140: 6715.71\n",
      "mean_20_dow2_2017: 6438.79\n",
      "max_7_sales: 6286.49\n",
      "sum_21_promo: 6249.33\n",
      "mean_20_dow4_2017: 6047.94\n",
      "has_promo_days_in_last_60: 5872.47\n",
      "has_sales_days_in_last_60: 5827.85\n",
      "has_sales_days_in_last_30: 5741.21\n",
      "std_6_sales: 5640.22\n",
      "std_3_sales: 5433.02\n",
      "max_60_sales: 5421.18\n",
      "std_4_sales: 5379.06\n",
      "sum_2_promo: 5356.19\n",
      "median_21_sales: 5133.05\n",
      "lag_30_sales: 5088.50\n",
      "median_4_sales: 5081.72\n",
      "last_has_promo_day_in_last_14: 4394.91\n",
      "max_21_sales: 4352.58\n",
      "lag_16_sales: 4323.86\n",
      "promo_14: 4272.19\n",
      "max_4_sales: 3961.51\n",
      "mean_20_dow3_2017: 3932.27\n",
      "max_30_sales: 3611.45\n",
      "lag_3_sales: 3472.38\n",
      "sum_13_promo_test: 3274.25\n",
      "sum_4_promo: 3235.86\n",
      "sum_5_promo_test: 3183.45\n",
      "last_has_promo_day_in_last_60: 3175.57\n",
      "min_5_sales: 3152.74\n",
      "sum_14_promo_test: 3142.60\n",
      "has_sales_days_in_last_14: 3123.62\n",
      "min_4_sales: 3115.02\n",
      "mean_20_dow5_2017: 2866.77\n",
      "last_has_sales_day_in_last_60: 2727.62\n",
      "last_has_promo_day_in_last_140: 2687.04\n",
      "lag_21_sales: 2685.16\n",
      "median_5_sales: 2669.25\n",
      "lag_13_sales: 2467.26\n",
      "last_has_promo_day_in_last_30: 2388.55\n",
      "mean_4_dow4_2017: 2332.07\n",
      "sum_7_promo_test: 2266.69\n",
      "has_sales_days_in_last_7: 2249.47\n",
      "last_has_sales_day_in_last_30: 2224.31\n",
      "sum_15_promo_test: 2202.68\n",
      "lag_15_sales: 2147.60\n",
      "mean_4_dow5_2017: 2142.43\n",
      "lag_8_sales: 2059.14\n",
      "sum_8_promo_test: 2030.65\n",
      "lag_6_sales: 1796.86\n",
      "mean_4_dow1_2017: 1790.86\n",
      "store_state_features: 1786.68\n",
      "max_6_sales: 1778.71\n",
      "mean_4_dow2_2017: 1758.74\n",
      "lag_4_sales: 1756.22\n",
      "lag_11_sales: 1745.62\n",
      "lag_5_sales: 1655.51\n",
      "lag_60_sales: 1590.12\n",
      "lag_12_sales: 1577.74\n",
      "mean_4_dow3_2017: 1568.15\n",
      "lag_9_sales: 1508.27\n",
      "sum_9_promo_test: 1355.56\n",
      "sum_2_promo_test: 1322.16\n",
      "lag_10_sales: 1311.52\n",
      "sum_1_promo_test: 1245.98\n",
      "sum_4_promo_test: 1231.28\n",
      "min_7_sales: 1101.66\n",
      "sum_11_promo_test: 1006.97\n",
      "sum_3_promo_test: 819.07\n",
      "sum_12_promo_test: 774.91\n",
      "sum_10_promo_test: 610.41\n",
      "promo_13: 607.14\n",
      "promo_6: 576.36\n",
      "min_14_sales: 420.43\n",
      "sum_6_promo: 363.75\n",
      "min_21_sales: 357.73\n",
      "min_6_sales: 355.71\n",
      "promo_15: 350.68\n",
      "sum_3_promo: 267.25\n",
      "promo_2: 233.64\n",
      "min_60_sales: 211.60\n",
      "promo_1: 163.74\n",
      "min_30_sales: 112.38\n",
      "sum_5_promo: 109.38\n",
      "last_has_sales_day_in_last_140: 82.32\n",
      "promo_4: 75.14\n",
      "promo_8: 62.65\n",
      "promo_9: 61.86\n",
      "promo_10: 40.90\n",
      "promo_5: 37.81\n",
      "promo_12: 29.82\n",
      "promo_3: 21.12\n",
      "sum_0_promo_test: 0.00\n",
      "promo_11: 0.00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  6%|▋         | 1/16 [02:28<37:01, 148.09s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1]\ttraining's l2: 0.941742\tvalid_1's l2: 0.923414\n",
      "Training until validation scores don't improve for 50 rounds.\n",
      "[2]\ttraining's l2: 0.882799\tvalid_1's l2: 0.865569\n",
      "[3]\ttraining's l2: 0.82957\tvalid_1's l2: 0.81349\n",
      "[4]\ttraining's l2: 0.781429\tvalid_1's l2: 0.766275\n",
      "[5]\ttraining's l2: 0.737955\tvalid_1's l2: 0.723719\n",
      "[6]\ttraining's l2: 0.698657\tvalid_1's l2: 0.685127\n",
      "[7]\ttraining's l2: 0.663155\tvalid_1's l2: 0.650251\n",
      "[8]\ttraining's l2: 0.630953\tvalid_1's l2: 0.618623\n",
      "[9]\ttraining's l2: 0.601902\tvalid_1's l2: 0.590107\n",
      "[10]\ttraining's l2: 0.575501\tvalid_1's l2: 0.564239\n",
      "[11]\ttraining's l2: 0.551655\tvalid_1's l2: 0.5409\n",
      "[12]\ttraining's l2: 0.530085\tvalid_1's l2: 0.519782\n",
      "[13]\ttraining's l2: 0.510609\tvalid_1's l2: 0.50062\n",
      "[14]\ttraining's l2: 0.49297\tvalid_1's l2: 0.483426\n",
      "[15]\ttraining's l2: 0.476968\tvalid_1's l2: 0.467783\n",
      "[16]\ttraining's l2: 0.462499\tvalid_1's l2: 0.453626\n",
      "[17]\ttraining's l2: 0.44945\tvalid_1's l2: 0.44085\n",
      "[18]\ttraining's l2: 0.437575\tvalid_1's l2: 0.429268\n",
      "[19]\ttraining's l2: 0.426864\tvalid_1's l2: 0.418817\n",
      "[20]\ttraining's l2: 0.417132\tvalid_1's l2: 0.40932\n",
      "[21]\ttraining's l2: 0.408314\tvalid_1's l2: 0.400748\n",
      "[22]\ttraining's l2: 0.400277\tvalid_1's l2: 0.392955\n",
      "[23]\ttraining's l2: 0.393028\tvalid_1's l2: 0.385944\n",
      "[24]\ttraining's l2: 0.386458\tvalid_1's l2: 0.379584\n",
      "[25]\ttraining's l2: 0.380483\tvalid_1's l2: 0.37383\n",
      "[26]\ttraining's l2: 0.375039\tvalid_1's l2: 0.368558\n",
      "[27]\ttraining's l2: 0.370088\tvalid_1's l2: 0.363834\n",
      "[28]\ttraining's l2: 0.365614\tvalid_1's l2: 0.359581\n",
      "[29]\ttraining's l2: 0.361565\tvalid_1's l2: 0.355674\n",
      "[30]\ttraining's l2: 0.357832\tvalid_1's l2: 0.352026\n",
      "[31]\ttraining's l2: 0.354464\tvalid_1's l2: 0.348792\n",
      "[32]\ttraining's l2: 0.351427\tvalid_1's l2: 0.345943\n",
      "[33]\ttraining's l2: 0.348636\tvalid_1's l2: 0.343311\n",
      "[34]\ttraining's l2: 0.346078\tvalid_1's l2: 0.340905\n",
      "[35]\ttraining's l2: 0.343749\tvalid_1's l2: 0.33868\n",
      "[36]\ttraining's l2: 0.341648\tvalid_1's l2: 0.336681\n",
      "[37]\ttraining's l2: 0.339716\tvalid_1's l2: 0.334879\n",
      "[38]\ttraining's l2: 0.337936\tvalid_1's l2: 0.333211\n",
      "[39]\ttraining's l2: 0.336297\tvalid_1's l2: 0.331724\n",
      "[40]\ttraining's l2: 0.334801\tvalid_1's l2: 0.330303\n",
      "[41]\ttraining's l2: 0.33342\tvalid_1's l2: 0.329032\n",
      "[42]\ttraining's l2: 0.332174\tvalid_1's l2: 0.327887\n",
      "[43]\ttraining's l2: 0.331019\tvalid_1's l2: 0.326871\n",
      "[44]\ttraining's l2: 0.329945\tvalid_1's l2: 0.325913\n",
      "[45]\ttraining's l2: 0.328962\tvalid_1's l2: 0.325013\n",
      "[46]\ttraining's l2: 0.328051\tvalid_1's l2: 0.32418\n",
      "[47]\ttraining's l2: 0.327243\tvalid_1's l2: 0.323461\n",
      "[48]\ttraining's l2: 0.326458\tvalid_1's l2: 0.322805\n",
      "[49]\ttraining's l2: 0.325757\tvalid_1's l2: 0.322233\n",
      "[50]\ttraining's l2: 0.325086\tvalid_1's l2: 0.321623\n",
      "[51]\ttraining's l2: 0.324454\tvalid_1's l2: 0.321072\n",
      "[52]\ttraining's l2: 0.323842\tvalid_1's l2: 0.320606\n",
      "[53]\ttraining's l2: 0.32331\tvalid_1's l2: 0.32019\n",
      "[54]\ttraining's l2: 0.322756\tvalid_1's l2: 0.319697\n",
      "[55]\ttraining's l2: 0.322262\tvalid_1's l2: 0.319309\n",
      "[56]\ttraining's l2: 0.321826\tvalid_1's l2: 0.318975\n",
      "[57]\ttraining's l2: 0.321389\tvalid_1's l2: 0.318653\n",
      "[58]\ttraining's l2: 0.321017\tvalid_1's l2: 0.318317\n",
      "[59]\ttraining's l2: 0.320649\tvalid_1's l2: 0.31804\n",
      "[60]\ttraining's l2: 0.320324\tvalid_1's l2: 0.317786\n",
      "[61]\ttraining's l2: 0.319987\tvalid_1's l2: 0.317534\n",
      "[62]\ttraining's l2: 0.319689\tvalid_1's l2: 0.31735\n",
      "[63]\ttraining's l2: 0.319378\tvalid_1's l2: 0.317108\n",
      "[64]\ttraining's l2: 0.319071\tvalid_1's l2: 0.316891\n",
      "[65]\ttraining's l2: 0.31878\tvalid_1's l2: 0.316715\n",
      "[66]\ttraining's l2: 0.318515\tvalid_1's l2: 0.316524\n",
      "[67]\ttraining's l2: 0.318256\tvalid_1's l2: 0.316331\n",
      "[68]\ttraining's l2: 0.317991\tvalid_1's l2: 0.316158\n",
      "[69]\ttraining's l2: 0.31779\tvalid_1's l2: 0.316044\n",
      "[70]\ttraining's l2: 0.317561\tvalid_1's l2: 0.315901\n",
      "[71]\ttraining's l2: 0.317361\tvalid_1's l2: 0.315759\n",
      "[72]\ttraining's l2: 0.317179\tvalid_1's l2: 0.315619\n",
      "[73]\ttraining's l2: 0.31699\tvalid_1's l2: 0.315517\n",
      "[74]\ttraining's l2: 0.316817\tvalid_1's l2: 0.315388\n",
      "[75]\ttraining's l2: 0.316638\tvalid_1's l2: 0.315303\n",
      "[76]\ttraining's l2: 0.316474\tvalid_1's l2: 0.315218\n",
      "[77]\ttraining's l2: 0.316285\tvalid_1's l2: 0.315141\n",
      "[78]\ttraining's l2: 0.316123\tvalid_1's l2: 0.315058\n",
      "[79]\ttraining's l2: 0.315965\tvalid_1's l2: 0.314975\n",
      "[80]\ttraining's l2: 0.3158\tvalid_1's l2: 0.314902\n",
      "[81]\ttraining's l2: 0.315629\tvalid_1's l2: 0.314826\n",
      "[82]\ttraining's l2: 0.315447\tvalid_1's l2: 0.314719\n",
      "[83]\ttraining's l2: 0.315311\tvalid_1's l2: 0.314677\n",
      "[84]\ttraining's l2: 0.315154\tvalid_1's l2: 0.314605\n",
      "[85]\ttraining's l2: 0.315\tvalid_1's l2: 0.314515\n",
      "[86]\ttraining's l2: 0.314842\tvalid_1's l2: 0.314445\n",
      "[87]\ttraining's l2: 0.31469\tvalid_1's l2: 0.314337\n",
      "[88]\ttraining's l2: 0.314548\tvalid_1's l2: 0.31429\n",
      "[89]\ttraining's l2: 0.314402\tvalid_1's l2: 0.31419\n",
      "[90]\ttraining's l2: 0.314254\tvalid_1's l2: 0.314084\n",
      "[91]\ttraining's l2: 0.31416\tvalid_1's l2: 0.314012\n",
      "[92]\ttraining's l2: 0.314059\tvalid_1's l2: 0.313976\n",
      "[93]\ttraining's l2: 0.313969\tvalid_1's l2: 0.313953\n",
      "[94]\ttraining's l2: 0.313815\tvalid_1's l2: 0.313816\n",
      "[95]\ttraining's l2: 0.313719\tvalid_1's l2: 0.313766\n",
      "[96]\ttraining's l2: 0.313623\tvalid_1's l2: 0.313729\n",
      "[97]\ttraining's l2: 0.313521\tvalid_1's l2: 0.313648\n",
      "[98]\ttraining's l2: 0.31339\tvalid_1's l2: 0.313588\n",
      "[99]\ttraining's l2: 0.313288\tvalid_1's l2: 0.313567\n",
      "[100]\ttraining's l2: 0.313181\tvalid_1's l2: 0.313504\n",
      "[101]\ttraining's l2: 0.313084\tvalid_1's l2: 0.313479\n",
      "[102]\ttraining's l2: 0.312985\tvalid_1's l2: 0.313412\n",
      "[103]\ttraining's l2: 0.312908\tvalid_1's l2: 0.313388\n",
      "[104]\ttraining's l2: 0.312849\tvalid_1's l2: 0.313363\n",
      "[105]\ttraining's l2: 0.312768\tvalid_1's l2: 0.313322\n",
      "[106]\ttraining's l2: 0.312676\tvalid_1's l2: 0.313279\n",
      "[107]\ttraining's l2: 0.312583\tvalid_1's l2: 0.313248\n",
      "[108]\ttraining's l2: 0.312511\tvalid_1's l2: 0.313224\n",
      "[109]\ttraining's l2: 0.312425\tvalid_1's l2: 0.313218\n",
      "[110]\ttraining's l2: 0.312347\tvalid_1's l2: 0.313188\n",
      "[111]\ttraining's l2: 0.312263\tvalid_1's l2: 0.313154\n",
      "[112]\ttraining's l2: 0.312173\tvalid_1's l2: 0.313135\n",
      "[113]\ttraining's l2: 0.312068\tvalid_1's l2: 0.313098\n",
      "[114]\ttraining's l2: 0.312001\tvalid_1's l2: 0.31306\n",
      "[115]\ttraining's l2: 0.311921\tvalid_1's l2: 0.313054\n",
      "[116]\ttraining's l2: 0.311778\tvalid_1's l2: 0.312961\n",
      "[117]\ttraining's l2: 0.31164\tvalid_1's l2: 0.312882\n",
      "[118]\ttraining's l2: 0.311519\tvalid_1's l2: 0.312783\n",
      "[119]\ttraining's l2: 0.311457\tvalid_1's l2: 0.312772\n",
      "[120]\ttraining's l2: 0.311393\tvalid_1's l2: 0.312755\n",
      "[121]\ttraining's l2: 0.311295\tvalid_1's l2: 0.312685\n",
      "[122]\ttraining's l2: 0.311198\tvalid_1's l2: 0.312616\n",
      "[123]\ttraining's l2: 0.311137\tvalid_1's l2: 0.312609\n",
      "[124]\ttraining's l2: 0.311033\tvalid_1's l2: 0.312545\n",
      "[125]\ttraining's l2: 0.310983\tvalid_1's l2: 0.31254\n",
      "[126]\ttraining's l2: 0.310922\tvalid_1's l2: 0.312519\n",
      "[127]\ttraining's l2: 0.310833\tvalid_1's l2: 0.312455\n",
      "[128]\ttraining's l2: 0.310768\tvalid_1's l2: 0.312458\n",
      "[129]\ttraining's l2: 0.310707\tvalid_1's l2: 0.312436\n",
      "[130]\ttraining's l2: 0.310646\tvalid_1's l2: 0.31241\n",
      "[131]\ttraining's l2: 0.31059\tvalid_1's l2: 0.312375\n",
      "[132]\ttraining's l2: 0.310533\tvalid_1's l2: 0.312363\n",
      "[133]\ttraining's l2: 0.310475\tvalid_1's l2: 0.312355\n",
      "[134]\ttraining's l2: 0.310406\tvalid_1's l2: 0.312341\n",
      "[135]\ttraining's l2: 0.310345\tvalid_1's l2: 0.312337\n",
      "[136]\ttraining's l2: 0.310274\tvalid_1's l2: 0.312296\n",
      "[137]\ttraining's l2: 0.310211\tvalid_1's l2: 0.312292\n",
      "[138]\ttraining's l2: 0.310135\tvalid_1's l2: 0.312246\n",
      "[139]\ttraining's l2: 0.310062\tvalid_1's l2: 0.312219\n",
      "[140]\ttraining's l2: 0.309978\tvalid_1's l2: 0.312186\n",
      "[141]\ttraining's l2: 0.30993\tvalid_1's l2: 0.312175\n",
      "[142]\ttraining's l2: 0.309865\tvalid_1's l2: 0.312161\n",
      "[143]\ttraining's l2: 0.309828\tvalid_1's l2: 0.312153\n",
      "[144]\ttraining's l2: 0.309776\tvalid_1's l2: 0.31213\n",
      "[145]\ttraining's l2: 0.309744\tvalid_1's l2: 0.312118\n",
      "[146]\ttraining's l2: 0.309696\tvalid_1's l2: 0.312112\n",
      "[147]\ttraining's l2: 0.309631\tvalid_1's l2: 0.312019\n",
      "[148]\ttraining's l2: 0.309589\tvalid_1's l2: 0.312015\n",
      "[149]\ttraining's l2: 0.309534\tvalid_1's l2: 0.311996\n",
      "[150]\ttraining's l2: 0.309458\tvalid_1's l2: 0.31197\n",
      "[151]\ttraining's l2: 0.309378\tvalid_1's l2: 0.311931\n",
      "[152]\ttraining's l2: 0.309297\tvalid_1's l2: 0.311882\n",
      "[153]\ttraining's l2: 0.30924\tvalid_1's l2: 0.31188\n",
      "[154]\ttraining's l2: 0.309158\tvalid_1's l2: 0.311853\n",
      "[155]\ttraining's l2: 0.309105\tvalid_1's l2: 0.311853\n",
      "[156]\ttraining's l2: 0.309059\tvalid_1's l2: 0.311846\n",
      "[157]\ttraining's l2: 0.309006\tvalid_1's l2: 0.311834\n",
      "[158]\ttraining's l2: 0.308966\tvalid_1's l2: 0.311823\n",
      "[159]\ttraining's l2: 0.308903\tvalid_1's l2: 0.311806\n",
      "[160]\ttraining's l2: 0.308852\tvalid_1's l2: 0.311808\n",
      "[161]\ttraining's l2: 0.308782\tvalid_1's l2: 0.31178\n",
      "[162]\ttraining's l2: 0.308728\tvalid_1's l2: 0.311774\n",
      "[163]\ttraining's l2: 0.308682\tvalid_1's l2: 0.311775\n",
      "[164]\ttraining's l2: 0.308651\tvalid_1's l2: 0.311781\n",
      "[165]\ttraining's l2: 0.308605\tvalid_1's l2: 0.311772\n",
      "[166]\ttraining's l2: 0.308559\tvalid_1's l2: 0.311759\n",
      "[167]\ttraining's l2: 0.308511\tvalid_1's l2: 0.311751\n",
      "[168]\ttraining's l2: 0.308464\tvalid_1's l2: 0.311735\n",
      "[169]\ttraining's l2: 0.308409\tvalid_1's l2: 0.311721\n",
      "[170]\ttraining's l2: 0.308361\tvalid_1's l2: 0.311706\n",
      "[171]\ttraining's l2: 0.308312\tvalid_1's l2: 0.311699\n",
      "[172]\ttraining's l2: 0.30826\tvalid_1's l2: 0.311703\n",
      "[173]\ttraining's l2: 0.308222\tvalid_1's l2: 0.311682\n",
      "[174]\ttraining's l2: 0.308189\tvalid_1's l2: 0.311692\n",
      "[175]\ttraining's l2: 0.308143\tvalid_1's l2: 0.311672\n",
      "[176]\ttraining's l2: 0.308076\tvalid_1's l2: 0.311645\n",
      "[177]\ttraining's l2: 0.308027\tvalid_1's l2: 0.311654\n",
      "[178]\ttraining's l2: 0.307989\tvalid_1's l2: 0.311651\n",
      "[179]\ttraining's l2: 0.307923\tvalid_1's l2: 0.311606\n",
      "[180]\ttraining's l2: 0.307875\tvalid_1's l2: 0.311602\n",
      "[181]\ttraining's l2: 0.30784\tvalid_1's l2: 0.311585\n",
      "[182]\ttraining's l2: 0.307788\tvalid_1's l2: 0.311577\n",
      "[183]\ttraining's l2: 0.307757\tvalid_1's l2: 0.311576\n",
      "[184]\ttraining's l2: 0.307704\tvalid_1's l2: 0.311574\n",
      "[185]\ttraining's l2: 0.307663\tvalid_1's l2: 0.311572\n",
      "[186]\ttraining's l2: 0.307613\tvalid_1's l2: 0.311554\n",
      "[187]\ttraining's l2: 0.307553\tvalid_1's l2: 0.311525\n",
      "[188]\ttraining's l2: 0.307506\tvalid_1's l2: 0.311519\n",
      "[189]\ttraining's l2: 0.307473\tvalid_1's l2: 0.311521\n",
      "[190]\ttraining's l2: 0.307447\tvalid_1's l2: 0.311521\n",
      "[191]\ttraining's l2: 0.307404\tvalid_1's l2: 0.311498\n",
      "[192]\ttraining's l2: 0.307352\tvalid_1's l2: 0.31149\n",
      "[193]\ttraining's l2: 0.307308\tvalid_1's l2: 0.31148\n",
      "[194]\ttraining's l2: 0.307269\tvalid_1's l2: 0.311468\n",
      "[195]\ttraining's l2: 0.307239\tvalid_1's l2: 0.311465\n",
      "[196]\ttraining's l2: 0.307202\tvalid_1's l2: 0.311456\n",
      "[197]\ttraining's l2: 0.307159\tvalid_1's l2: 0.311452\n",
      "[198]\ttraining's l2: 0.307128\tvalid_1's l2: 0.311444\n",
      "[199]\ttraining's l2: 0.307086\tvalid_1's l2: 0.311434\n",
      "[200]\ttraining's l2: 0.307043\tvalid_1's l2: 0.311426\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[200]\ttraining's l2: 0.307043\tvalid_1's l2: 0.311426\n",
      "mean_14_sales: 7368881.51\n",
      "mean_7_sales: 4803844.84\n",
      "mean_6_sales: 985048.76\n",
      "mean_20_dow1_2017: 237191.58\n",
      "sum_1_promo_test: 235960.62\n",
      "mean_30_sales: 227954.51\n",
      "item_class_features: 202880.46\n",
      "mean_21_sales: 153167.59\n",
      "median_6_sales: 146144.14\n",
      "median_60_sales: 117136.33\n",
      "mean_60_sales: 90926.79\n",
      "median_30_sales: 87634.00\n",
      "std_14_sales: 84939.56\n",
      "promo_1: 59673.94\n",
      "has_promo_days_in_last_7: 57527.72\n",
      "lag_1_sales: 54386.74\n",
      "has_promo_days_in_last_14: 45034.60\n",
      "mean_5_sales: 30381.08\n",
      "mean_4_dow1_2017: 26597.67\n",
      "mean_20_dow2_2017: 23622.38\n",
      "store_city_features: 23162.04\n",
      "last_has_sales_day_in_last_7: 22760.25\n",
      "sum_7_promo_test: 20245.62\n",
      "promo_0: 18811.73\n",
      "store_cluster_features: 18612.04\n",
      "median_14_sales: 17342.21\n",
      "mean_3_sales: 17274.12\n",
      "has_sales_days_in_last_140: 16517.98\n",
      "sum_6_promo_test: 16395.07\n",
      "std_21_sales: 15914.61\n",
      "mean_4_dow6_2017: 15718.95\n",
      "last_has_sales_day_in_last_14: 13487.67\n",
      "std_30_sales: 13280.47\n",
      "has_promo_days_in_last_30: 12724.76\n",
      "std_60_sales: 11836.44\n",
      "sum_5_promo_test: 11298.55\n",
      "lag_30_sales: 10899.91\n",
      "std_7_sales: 10444.84\n",
      "sum_2_promo_test: 10198.12\n",
      "item_family_features: 9807.65\n",
      "mean_20_dow6_2017: 9688.62\n",
      "has_sales_days_in_last_60: 9009.06\n",
      "max_6_sales: 8403.72\n",
      "mean_4_sales: 8238.63\n",
      "mean_4_dow2_2017: 7977.63\n",
      "sum_2_promo: 7658.42\n",
      "has_promo_days_in_last_60: 7654.99\n",
      "std_6_sales: 7632.78\n",
      "max_3_sales: 7619.09\n",
      "sum_8_promo_test: 7510.62\n",
      "mean_20_dow4_2017: 7348.71\n",
      "sum_15_promo_test: 7337.99\n",
      "has_sales_days_in_last_30: 7173.15\n",
      "max_4_sales: 6758.12\n",
      "mean_20_dow0_2017: 6632.72\n",
      "median_7_sales: 6482.35\n",
      "lag_60_sales: 6208.41\n",
      "max_60_sales: 5972.15\n",
      "max_5_sales: 5479.96\n",
      "has_promo_days_in_last_140: 5476.73\n",
      "median_3_sales: 5292.18\n",
      "std_5_sales: 5236.70\n",
      "lag_5_sales: 4847.79\n",
      "lag_13_sales: 4694.19\n",
      "sum_14_promo_test: 4617.71\n",
      "sum_4_promo: 4568.33\n",
      "last_has_sales_day_in_last_30: 4468.58\n",
      "lag_6_sales: 4436.28\n",
      "max_14_sales: 4426.72\n",
      "has_sales_days_in_last_14: 4400.68\n",
      "sum_7_promo: 4381.06\n",
      "max_7_sales: 4240.80\n",
      "last_has_promo_day_in_last_30: 4109.71\n",
      "std_4_sales: 4079.02\n",
      "max_30_sales: 3860.56\n",
      "last_has_promo_day_in_last_7: 3755.03\n",
      "mean_4_dow4_2017: 3617.16\n",
      "std_3_sales: 3564.46\n",
      "median_5_sales: 3446.03\n",
      "mean_4_dow0_2017: 3372.73\n",
      "max_21_sales: 3347.53\n",
      "last_has_promo_day_in_last_14: 3068.93\n",
      "lag_2_sales: 3011.97\n",
      "lag_15_sales: 3007.77\n",
      "lag_12_sales: 2817.87\n",
      "sum_13_promo_test: 2714.65\n",
      "mean_20_dow5_2017: 2703.12\n",
      "median_4_sales: 2623.95\n",
      "min_6_sales: 2621.55\n",
      "min_5_sales: 2507.71\n",
      "store_type_features: 2425.14\n",
      "sum_3_promo_test: 2412.14\n",
      "lag_11_sales: 2346.31\n",
      "median_21_sales: 2317.57\n",
      "mean_20_dow3_2017: 2241.11\n",
      "last_has_sales_day_in_last_60: 2223.34\n",
      "store_state_features: 2199.97\n",
      "lag_7_sales: 2070.81\n",
      "lag_4_sales: 2060.65\n",
      "last_has_sales_day_in_last_140: 2008.75\n",
      "mean_4_dow5_2017: 1971.29\n",
      "min_4_sales: 1913.48\n",
      "lag_8_sales: 1895.26\n",
      "sum_10_promo_test: 1759.57\n",
      "mean_4_dow3_2017: 1756.61\n",
      "last_has_promo_day_in_last_140: 1749.65\n",
      "lag_14_sales: 1735.93\n",
      "lag_9_sales: 1734.71\n",
      "lag_10_sales: 1698.93\n",
      "lag_21_sales: 1636.37\n",
      "lag_3_sales: 1631.81\n",
      "sum_4_promo_test: 1596.33\n",
      "min_3_sales: 1532.05\n",
      "lag_16_sales: 1463.82\n",
      "sum_9_promo_test: 1458.71\n",
      "last_has_promo_day_in_last_60: 1277.13\n",
      "has_sales_days_in_last_7: 1055.96\n",
      "sum_6_promo: 1024.82\n",
      "promo_7: 1021.44\n",
      "min_7_sales: 997.05\n",
      "sum_12_promo_test: 972.50\n",
      "promo_8: 957.39\n",
      "sum_11_promo_test: 930.32\n",
      "sum_14_promo: 892.92\n",
      "promo_3: 890.60\n",
      "promo_4: 705.57\n",
      "promo_2: 526.47\n",
      "promo_14: 463.96\n",
      "promo_15: 384.26\n",
      "sum_21_promo: 370.65\n",
      "sum_3_promo: 362.27\n",
      "sum_5_promo: 333.88\n",
      "promo_6: 274.49\n",
      "promo_9: 246.65\n",
      "min_30_sales: 184.73\n",
      "promo_13: 161.97\n",
      "min_14_sales: 132.34\n",
      "min_21_sales: 124.40\n",
      "promo_12: 56.94\n",
      "min_60_sales: 55.82\n",
      "promo_10: 24.78\n",
      "promo_5: 17.73\n",
      "promo_11: 14.97\n",
      "sum_0_promo_test: 0.00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 12%|█▎        | 2/16 [04:47<33:57, 145.52s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1]\ttraining's l2: 1.04939\tvalid_1's l2: 1.05788\n",
      "Training until validation scores don't improve for 50 rounds.\n",
      "[2]\ttraining's l2: 0.981186\tvalid_1's l2: 0.989747\n",
      "[3]\ttraining's l2: 0.919235\tvalid_1's l2: 0.927866\n",
      "[4]\ttraining's l2: 0.863157\tvalid_1's l2: 0.871762\n",
      "[5]\ttraining's l2: 0.812775\tvalid_1's l2: 0.821429\n",
      "[6]\ttraining's l2: 0.767725\tvalid_1's l2: 0.776221\n",
      "[7]\ttraining's l2: 0.726372\tvalid_1's l2: 0.734886\n",
      "[8]\ttraining's l2: 0.68876\tvalid_1's l2: 0.697186\n",
      "[9]\ttraining's l2: 0.65494\tvalid_1's l2: 0.663249\n",
      "[10]\ttraining's l2: 0.624298\tvalid_1's l2: 0.632503\n",
      "[11]\ttraining's l2: 0.596324\tvalid_1's l2: 0.604548\n",
      "[12]\ttraining's l2: 0.571092\tvalid_1's l2: 0.579292\n",
      "[13]\ttraining's l2: 0.548192\tvalid_1's l2: 0.556343\n",
      "[14]\ttraining's l2: 0.527553\tvalid_1's l2: 0.535632\n",
      "[15]\ttraining's l2: 0.509003\tvalid_1's l2: 0.516978\n",
      "[16]\ttraining's l2: 0.491866\tvalid_1's l2: 0.499768\n",
      "[17]\ttraining's l2: 0.476408\tvalid_1's l2: 0.484243\n",
      "[18]\ttraining's l2: 0.462359\tvalid_1's l2: 0.470189\n",
      "[19]\ttraining's l2: 0.44968\tvalid_1's l2: 0.45754\n",
      "[20]\ttraining's l2: 0.438329\tvalid_1's l2: 0.446069\n",
      "[21]\ttraining's l2: 0.427962\tvalid_1's l2: 0.435617\n",
      "[22]\ttraining's l2: 0.418431\tvalid_1's l2: 0.42605\n",
      "[23]\ttraining's l2: 0.409813\tvalid_1's l2: 0.417356\n",
      "[24]\ttraining's l2: 0.40197\tvalid_1's l2: 0.409427\n",
      "[25]\ttraining's l2: 0.394892\tvalid_1's l2: 0.402307\n",
      "[26]\ttraining's l2: 0.388416\tvalid_1's l2: 0.395761\n",
      "[27]\ttraining's l2: 0.382484\tvalid_1's l2: 0.389758\n",
      "[28]\ttraining's l2: 0.377153\tvalid_1's l2: 0.384393\n",
      "[29]\ttraining's l2: 0.372314\tvalid_1's l2: 0.379503\n",
      "[30]\ttraining's l2: 0.36791\tvalid_1's l2: 0.375092\n",
      "[31]\ttraining's l2: 0.363893\tvalid_1's l2: 0.371039\n",
      "[32]\ttraining's l2: 0.360261\tvalid_1's l2: 0.367437\n",
      "[33]\ttraining's l2: 0.356904\tvalid_1's l2: 0.364071\n",
      "[34]\ttraining's l2: 0.353804\tvalid_1's l2: 0.360962\n",
      "[35]\ttraining's l2: 0.351042\tvalid_1's l2: 0.35819\n",
      "[36]\ttraining's l2: 0.348478\tvalid_1's l2: 0.355609\n",
      "[37]\ttraining's l2: 0.346169\tvalid_1's l2: 0.35331\n",
      "[38]\ttraining's l2: 0.344044\tvalid_1's l2: 0.351193\n",
      "[39]\ttraining's l2: 0.342044\tvalid_1's l2: 0.349254\n",
      "[40]\ttraining's l2: 0.340235\tvalid_1's l2: 0.347402\n",
      "[41]\ttraining's l2: 0.338578\tvalid_1's l2: 0.345702\n",
      "[42]\ttraining's l2: 0.337087\tvalid_1's l2: 0.344159\n",
      "[43]\ttraining's l2: 0.335696\tvalid_1's l2: 0.342813\n",
      "[44]\ttraining's l2: 0.334409\tvalid_1's l2: 0.341604\n",
      "[45]\ttraining's l2: 0.333212\tvalid_1's l2: 0.340445\n",
      "[46]\ttraining's l2: 0.332173\tvalid_1's l2: 0.339379\n",
      "[47]\ttraining's l2: 0.331166\tvalid_1's l2: 0.338384\n",
      "[48]\ttraining's l2: 0.330212\tvalid_1's l2: 0.337499\n",
      "[49]\ttraining's l2: 0.329348\tvalid_1's l2: 0.336674\n",
      "[50]\ttraining's l2: 0.328576\tvalid_1's l2: 0.335909\n",
      "[51]\ttraining's l2: 0.32784\tvalid_1's l2: 0.335203\n",
      "[52]\ttraining's l2: 0.327126\tvalid_1's l2: 0.334601\n",
      "[53]\ttraining's l2: 0.326491\tvalid_1's l2: 0.334027\n",
      "[54]\ttraining's l2: 0.325851\tvalid_1's l2: 0.333414\n",
      "[55]\ttraining's l2: 0.325253\tvalid_1's l2: 0.332893\n",
      "[56]\ttraining's l2: 0.324751\tvalid_1's l2: 0.332459\n",
      "[57]\ttraining's l2: 0.324244\tvalid_1's l2: 0.332033\n",
      "[58]\ttraining's l2: 0.323776\tvalid_1's l2: 0.331587\n",
      "[59]\ttraining's l2: 0.323326\tvalid_1's l2: 0.331228\n",
      "[60]\ttraining's l2: 0.322903\tvalid_1's l2: 0.330895\n",
      "[61]\ttraining's l2: 0.322509\tvalid_1's l2: 0.330568\n",
      "[62]\ttraining's l2: 0.322135\tvalid_1's l2: 0.330266\n",
      "[63]\ttraining's l2: 0.321811\tvalid_1's l2: 0.33001\n",
      "[64]\ttraining's l2: 0.321461\tvalid_1's l2: 0.329757\n",
      "[65]\ttraining's l2: 0.321142\tvalid_1's l2: 0.329505\n",
      "[66]\ttraining's l2: 0.320834\tvalid_1's l2: 0.329275\n",
      "[67]\ttraining's l2: 0.320531\tvalid_1's l2: 0.329041\n",
      "[68]\ttraining's l2: 0.320205\tvalid_1's l2: 0.328842\n",
      "[69]\ttraining's l2: 0.319937\tvalid_1's l2: 0.328663\n",
      "[70]\ttraining's l2: 0.319703\tvalid_1's l2: 0.328501\n",
      "[71]\ttraining's l2: 0.319416\tvalid_1's l2: 0.328353\n",
      "[72]\ttraining's l2: 0.319194\tvalid_1's l2: 0.328203\n",
      "[73]\ttraining's l2: 0.318976\tvalid_1's l2: 0.328048\n",
      "[74]\ttraining's l2: 0.318753\tvalid_1's l2: 0.32792\n",
      "[75]\ttraining's l2: 0.318527\tvalid_1's l2: 0.327813\n",
      "[76]\ttraining's l2: 0.318308\tvalid_1's l2: 0.327675\n",
      "[77]\ttraining's l2: 0.31811\tvalid_1's l2: 0.327565\n",
      "[78]\ttraining's l2: 0.317912\tvalid_1's l2: 0.327448\n",
      "[79]\ttraining's l2: 0.317734\tvalid_1's l2: 0.327322\n",
      "[80]\ttraining's l2: 0.317553\tvalid_1's l2: 0.327229\n",
      "[81]\ttraining's l2: 0.317398\tvalid_1's l2: 0.32713\n",
      "[82]\ttraining's l2: 0.317222\tvalid_1's l2: 0.327018\n",
      "[83]\ttraining's l2: 0.317049\tvalid_1's l2: 0.326922\n",
      "[84]\ttraining's l2: 0.316896\tvalid_1's l2: 0.326837\n",
      "[85]\ttraining's l2: 0.316732\tvalid_1's l2: 0.32674\n",
      "[86]\ttraining's l2: 0.316571\tvalid_1's l2: 0.32664\n",
      "[87]\ttraining's l2: 0.316411\tvalid_1's l2: 0.326564\n",
      "[88]\ttraining's l2: 0.316287\tvalid_1's l2: 0.326496\n",
      "[89]\ttraining's l2: 0.316159\tvalid_1's l2: 0.326435\n",
      "[90]\ttraining's l2: 0.315998\tvalid_1's l2: 0.326327\n",
      "[91]\ttraining's l2: 0.315879\tvalid_1's l2: 0.326233\n",
      "[92]\ttraining's l2: 0.315768\tvalid_1's l2: 0.326181\n",
      "[93]\ttraining's l2: 0.315632\tvalid_1's l2: 0.326172\n",
      "[94]\ttraining's l2: 0.315524\tvalid_1's l2: 0.326109\n",
      "[95]\ttraining's l2: 0.315414\tvalid_1's l2: 0.326051\n",
      "[96]\ttraining's l2: 0.315285\tvalid_1's l2: 0.326026\n",
      "[97]\ttraining's l2: 0.315161\tvalid_1's l2: 0.325995\n",
      "[98]\ttraining's l2: 0.315034\tvalid_1's l2: 0.325934\n",
      "[99]\ttraining's l2: 0.314927\tvalid_1's l2: 0.325915\n",
      "[100]\ttraining's l2: 0.314809\tvalid_1's l2: 0.325869\n",
      "[101]\ttraining's l2: 0.314714\tvalid_1's l2: 0.325822\n",
      "[102]\ttraining's l2: 0.314548\tvalid_1's l2: 0.325794\n",
      "[103]\ttraining's l2: 0.314407\tvalid_1's l2: 0.325739\n",
      "[104]\ttraining's l2: 0.314323\tvalid_1's l2: 0.325709\n",
      "[105]\ttraining's l2: 0.314245\tvalid_1's l2: 0.325692\n",
      "[106]\ttraining's l2: 0.314111\tvalid_1's l2: 0.325602\n",
      "[107]\ttraining's l2: 0.313972\tvalid_1's l2: 0.325628\n",
      "[108]\ttraining's l2: 0.313871\tvalid_1's l2: 0.325601\n",
      "[109]\ttraining's l2: 0.313741\tvalid_1's l2: 0.325528\n",
      "[110]\ttraining's l2: 0.31367\tvalid_1's l2: 0.325495\n",
      "[111]\ttraining's l2: 0.313588\tvalid_1's l2: 0.325431\n",
      "[112]\ttraining's l2: 0.313494\tvalid_1's l2: 0.325411\n",
      "[113]\ttraining's l2: 0.313409\tvalid_1's l2: 0.325399\n",
      "[114]\ttraining's l2: 0.31333\tvalid_1's l2: 0.325394\n",
      "[115]\ttraining's l2: 0.313212\tvalid_1's l2: 0.325405\n",
      "[116]\ttraining's l2: 0.313144\tvalid_1's l2: 0.325388\n",
      "[117]\ttraining's l2: 0.313055\tvalid_1's l2: 0.325366\n",
      "[118]\ttraining's l2: 0.312965\tvalid_1's l2: 0.325365\n",
      "[119]\ttraining's l2: 0.312877\tvalid_1's l2: 0.325341\n",
      "[120]\ttraining's l2: 0.312758\tvalid_1's l2: 0.325302\n",
      "[121]\ttraining's l2: 0.312679\tvalid_1's l2: 0.325296\n",
      "[122]\ttraining's l2: 0.312605\tvalid_1's l2: 0.325278\n",
      "[123]\ttraining's l2: 0.312522\tvalid_1's l2: 0.32525\n",
      "[124]\ttraining's l2: 0.312449\tvalid_1's l2: 0.325247\n",
      "[125]\ttraining's l2: 0.312385\tvalid_1's l2: 0.32522\n",
      "[126]\ttraining's l2: 0.312304\tvalid_1's l2: 0.32521\n",
      "[127]\ttraining's l2: 0.312243\tvalid_1's l2: 0.325205\n",
      "[128]\ttraining's l2: 0.312168\tvalid_1's l2: 0.325208\n",
      "[129]\ttraining's l2: 0.312101\tvalid_1's l2: 0.325209\n",
      "[130]\ttraining's l2: 0.312034\tvalid_1's l2: 0.325186\n",
      "[131]\ttraining's l2: 0.311964\tvalid_1's l2: 0.325174\n"
     ]
    }
   ],
   "source": [
    "for i in tqdm(range(16)):\n",
    "    dtrain = lgb.Dataset(\n",
    "        X_train, label=y_train[:, i],\n",
    "        categorical_feature=cate_vars,\n",
    "        weight=pd.concat([items[\"perishable\"]] * nbr_weeks) * 0.25 + 1\n",
    "    )\n",
    "    dval = lgb.Dataset(\n",
    "        X_val, label=y_val[:, i], reference=dtrain,\n",
    "        weight=items[\"perishable\"] * 0.25 + 1,\n",
    "        categorical_feature=cate_vars)\n",
    "\n",
    "    bst = lgb.train(\n",
    "        params,\n",
    "        dtrain,\n",
    "        num_boost_round=MAX_ROUNDS,\n",
    "#         verbose_eval = False,\n",
    "        valid_sets=[dtrain, dval], early_stopping_rounds=50)\n",
    "    print(\"\\n\".join((\"%s: %.2f\" % x) for x in sorted(\n",
    "        zip(X_train.columns, bst.feature_importance(\"gain\")),\n",
    "        key=lambda x: x[1], reverse=True\n",
    "    )))\n",
    "    val_pred.append(bst.predict(\n",
    "        X_val, num_iteration=bst.best_iteration or MAX_ROUNDS))\n",
    "    test_pred.append(bst.predict(\n",
    "        X_test, num_iteration=bst.best_iteration or MAX_ROUNDS))\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generate submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.3511710550491811"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mse = mean_squared_error(y_val, np.array(val_pred).transpose())\n",
    "mse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.1507177033492823"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "252/1672"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/liuyu/anaconda3/lib/python3.7/site-packages/py4j/java_collections.py:13: DeprecationWarning: Using or importing the ABCs from 'collections' instead of from 'collections.abc' is deprecated, and in 3.8 it will stop working\n",
      "  from collections import (\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation mse: 0.3511710550491811\n"
     ]
    }
   ],
   "source": [
    "mse = mean_squared_error(y_val, np.array(val_pred).transpose())\n",
    "\n",
    "mlflow.set_experiment('grocery forecasting')\n",
    "with mlflow.start_run(run_name='lgbm'):\n",
    "    mlflow.log_param('model', 'lgbm')\n",
    "    mlflow.log_param('train starts', train_start)\n",
    "    mlflow.log_params(params)\n",
    "    mlflow.log_param('lagging', LAG_DICT.values())\n",
    "    mlflow.log_param('slidingWindows', SLIDING_DICT.values())\n",
    "    mlflow.log_param('item_info', 'Yes')\n",
    "    mlflow.log_param('store_info', 'Yes')\n",
    "    mlflow.log_param('private score', 0.52207)\n",
    "    mlflow.log_param('private rank', '15%')\n",
    "    mlflow.log_param('public score', 0.51485)\n",
    "\n",
    "    mlflow.log_metric('mse', mse)\n",
    "    \n",
    "print(\"Validation mse:\", mse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Making submission...\n"
     ]
    }
   ],
   "source": [
    "print(\"Making submission...\")\n",
    "y_test = np.array(test_pred).transpose()\n",
    "df_preds = pd.DataFrame(\n",
    "    y_test, index=df_train.index,\n",
    "    columns=pd.date_range(\"2017-08-16\", periods=16)\n",
    ").stack().to_frame(\"unit_sales\")\n",
    "df_preds.index.set_names([\"store_nbr\", \"item_nbr\", \"date\"], inplace=True)\n",
    "\n",
    "submission = df_test[[\"id\"]].join(df_preds, how=\"left\").fillna(0)\n",
    "submission[\"unit_sales\"] = np.clip(np.expm1(submission[\"unit_sales\"]), 0, 1000)\n",
    "submission.to_csv('lgb.csv', float_format='%.4f', index=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
